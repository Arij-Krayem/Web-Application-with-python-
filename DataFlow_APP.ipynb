{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxfRkAeBuNSm",
        "outputId": "c06c7d49-a633-4b9f-c1e3-2ff2a89b1083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.47.2-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Collecting gradio-client==1.13.3 (from gradio)\n",
            "  Downloading gradio_client-1.13.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.47.2-py3-none-any.whl (60.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.13.3-py3-none-any.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.4/325.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gradio-client, gradio\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.13.0\n",
            "    Uninstalling gradio_client-1.13.0:\n",
            "      Successfully uninstalled gradio_client-1.13.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.46.0\n",
            "    Uninstalling gradio-5.46.0:\n",
            "      Successfully uninstalled gradio-5.46.0\n",
            "Successfully installed gradio-5.47.2 gradio-client-1.13.3\n"
          ]
        }
      ],
      "source": [
        "!pip install -U gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAYPmkak4XxM",
        "outputId": "e7caac94-7db9-4648-847b-192f4dc8e04c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.47.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "pip install gradio pandas numpy plotly statsmodels scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y6bRRHA6uXk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68a4945c-2ea5-44a7-ecac-45d7a0d560f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TI8FN7r6ucQ7"
      },
      "outputs": [],
      "source": [
        "!pip install gradio pandas openpyxl matplotlib --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import os\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from datetime import datetime\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "import re\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# --- Global State ---\n",
        "original_df = pd.DataFrame()\n",
        "current_filtered_df = pd.DataFrame()\n",
        "preview_visible = False\n",
        "filtered_visible = False\n",
        "tables_data = {}  # Dictionary to store table data\n",
        "active_filter_values = {}  # column -> list of selected values (strings)\n",
        "filter_columns_map = {}  # Map each filter dropdown index -> column name (kept in sync with current dataset)\n",
        "\n",
        "# --- Constants ---\n",
        "EXCLUDED_STAT_COLS = ['year', 'crop_year', 'id', 'code', 'index', 'no']\n",
        "MAX_UNIQUE_FILTER = 500\n",
        "MAX_FILTERS = 10\n",
        "FILTER_ALL_TOKEN = \"All\"\n",
        "temp_dir = tempfile.gettempdir()\n",
        "\n",
        "# CSV/Excel settings (Excel-friendly)\n",
        "CSV_ENCODING = \"utf-8-sig\"   # BOM so Excel detects UTF-8 automatically\n",
        "CSV_DELIMITER = \";\"          # French/European Excel uses semicolon; set to \",\" if desired\n",
        "\n",
        "def write_csv(df: pd.DataFrame, path: str):\n",
        "    df.to_csv(path, index=False, encoding=CSV_ENCODING, sep=CSV_DELIMITER)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def sanitize_token(s: str, max_len: int = 120) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = str(s)\n",
        "    s = re.sub(r'\\s+', '_', s.strip())\n",
        "    s = re.sub(r'[^\\w\\-\\+\\=\\.,]', '', s)\n",
        "    return s[:max_len]\n",
        "\n",
        "def coerce_datetime_columns(df: pd.DataFrame, threshold=0.7) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    for c in out.columns:\n",
        "        if pd.api.types.is_datetime64_any_dtype(out[c]):\n",
        "            continue\n",
        "        if out[c].dtype == object:\n",
        "            parsed = pd.to_datetime(out[c], errors=\"coerce\")\n",
        "            if parsed.notna().mean() >= threshold:\n",
        "                out[c] = parsed\n",
        "    return out\n",
        "\n",
        "def robust_to_numeric(series_like) -> pd.Series:\n",
        "    s = pd.Series(series_like)\n",
        "    if pd.api.types.is_numeric_dtype(s):\n",
        "        return pd.to_numeric(s, errors='coerce')\n",
        "\n",
        "    s = s.astype(str).str.strip()\n",
        "    # Normalize non-breaking spaces and spaces\n",
        "    s = s.str.replace('\\u00A0', ' ', regex=False).str.replace('\\xa0', ' ', regex=False)\n",
        "    s = s.str.replace(r'\\s+', '', regex=True)\n",
        "\n",
        "    # Convert parentheses negatives: (123) -> -123\n",
        "    s = s.str.replace(r'^\\((.+)\\)$', r'-\\1', regex=True)\n",
        "\n",
        "    # Detect decimal comma or values with comma but no dot\n",
        "    dec_comma_pattern = r'^\\s*[-+]?\\d{1,3}(\\.\\d{3})*,\\d+$'\n",
        "    mask_decimal_comma = s.str.contains(dec_comma_pattern, regex=True) | (s.str.contains(',') & ~s.str.contains(r'\\.'))\n",
        "\n",
        "    s2 = s.copy()\n",
        "    s2.loc[mask_decimal_comma] = (\n",
        "        s2.loc[mask_decimal_comma]\n",
        "          .str.replace('.', '', regex=False)\n",
        "          .str.replace(',', '.', regex=False)\n",
        "    )\n",
        "    s2.loc[~mask_decimal_comma] = s2.loc[~mask_decimal_comma].str.replace(',', '', regex=False)\n",
        "    s2 = s2.str.replace(r'[^0-9\\.\\-\\+eE]', '', regex=True)\n",
        "    return pd.to_numeric(s2, errors='coerce')\n",
        "\n",
        "def safe_load_df(file):\n",
        "    file_path = file.name if hasattr(file, \"name\") else file\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        try:\n",
        "            # Auto-detect delimiter robustly\n",
        "            df = pd.read_csv(file_path, sep=None, engine='python', low_memory=False)\n",
        "        except Exception:\n",
        "            df = pd.read_csv(file_path, low_memory=False)\n",
        "    elif file_path.endswith((\".xlsx\", \".xls\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please upload a CSV or Excel file.\")\n",
        "    return coerce_datetime_columns(df)\n",
        "\n",
        "def visible_filter_options(col_name, unique_vals):\n",
        "    choices = [FILTER_ALL_TOKEN] + sorted(unique_vals)\n",
        "    if len(unique_vals) <= MAX_UNIQUE_FILTER:\n",
        "        return gr.update(visible=True, choices=choices, value=None, label=str(col_name), interactive=True)\n",
        "    return gr.update(visible=False, choices=[], value=None, label=str(col_name))\n",
        "\n",
        "def get_filter_suffix_for_filename(max_len: int = 120) -> str:\n",
        "    if not active_filter_values:\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    # Stable order by column name for reproducible filenames\n",
        "    for col in sorted(active_filter_values.keys()):\n",
        "        vals = active_filter_values[col]\n",
        "        if not vals:\n",
        "            continue\n",
        "        if FILTER_ALL_TOKEN in vals:\n",
        "            continue\n",
        "        col_tok = sanitize_token(col, max_len=50)\n",
        "        val_toks = [sanitize_token(v, max_len=50) for v in vals if v and v != FILTER_ALL_TOKEN]\n",
        "        if not val_toks:\n",
        "            continue\n",
        "        parts.append(f\"{col_tok}={'+'.join(val_toks)}\")\n",
        "    if not parts:\n",
        "        return \"\"\n",
        "    suffix = \"__\" + \"__\".join(parts)\n",
        "    if len(suffix) > max_len:\n",
        "        kept = []\n",
        "        total = 2\n",
        "        for p in parts:\n",
        "            if total + len(p) + 2 > max_len:\n",
        "                break\n",
        "            kept.append(p)\n",
        "            total += len(p) + 2\n",
        "        suffix = \"__\" + \"__\".join(kept) if kept else \"\"\n",
        "    return suffix\n",
        "\n",
        "def attach_static_filter_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if df is None or df.empty:\n",
        "        return df\n",
        "    out = df.copy()\n",
        "    for col, vals in active_filter_values.items():\n",
        "        if FILTER_ALL_TOKEN in (vals or []):\n",
        "            continue\n",
        "        value_str = \" | \".join([str(v) for v in (vals or [])]) if vals else \"\"\n",
        "        if value_str and col not in out.columns:\n",
        "            out[col] = value_str\n",
        "    return out\n",
        "\n",
        "def maybe_attach_static_cols(df: pd.DataFrame, meta_suffix: str) -> pd.DataFrame:\n",
        "    if meta_suffix:\n",
        "        return df\n",
        "    return attach_static_filter_cols(df)\n",
        "\n",
        "def make_temp_csv(df, prefix, meta_suffix: str = None):\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    meta = sanitize_token(meta_suffix or \"\", max_len=200)\n",
        "    filename = f\"{prefix}{meta}_{timestamp}.csv\" if meta else f\"{prefix}_{timestamp}.csv\"\n",
        "    path = os.path.join(temp_dir, filename)\n",
        "    try:\n",
        "        write_csv(df, path)\n",
        "        return path\n",
        "    except Exception:\n",
        "        filename = f\"{prefix}{meta}_{timestamp}_{np.random.randint(1000,9999)}.csv\" if meta else f\"{prefix}_{timestamp}_{np.random.randint(1000,9999)}.csv\"\n",
        "        path = os.path.join(temp_dir, filename)\n",
        "        write_csv(df, path)\n",
        "        return path\n",
        "\n",
        "def save_modified_table(table_name, modified_df):\n",
        "    global temp_dir\n",
        "    modified_dir = os.path.join(temp_dir, \"modified_tables\")\n",
        "    os.makedirs(modified_dir, exist_ok=True)\n",
        "    clean_name = re.sub(r'[^\\w\\-_]', '', str(table_name))\n",
        "    suffix = sanitize_token(get_filter_suffix_for_filename(), max_len=200)\n",
        "    csv_path = os.path.join(modified_dir, f\"{clean_name}{suffix}_modified.csv\")\n",
        "    try:\n",
        "        df_to_save = maybe_attach_static_cols(modified_df, suffix)\n",
        "        write_csv(df_to_save, csv_path)\n",
        "        return csv_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving modified table: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def handle_table_modification(table_name, modified_df):\n",
        "    global tables_data, current_filtered_df\n",
        "    if modified_df is not None and not modified_df.empty:\n",
        "        modified_df = coerce_datetime_columns(modified_df)\n",
        "        current_filtered_df = modified_df.copy()\n",
        "        tables_data[table_name] = current_filtered_df\n",
        "        csv_path = save_modified_table(table_name, modified_df)\n",
        "        meta = get_filter_suffix_for_filename()\n",
        "        cur_df = maybe_attach_static_cols(current_filtered_df, meta)\n",
        "        cur_path = make_temp_csv(cur_df, \"current_data\", meta)\n",
        "        if csv_path:\n",
        "            return gr.File(value=csv_path, visible=True), \"Table updated successfully\", gr.update(value=cur_path, visible=True)\n",
        "    return gr.File(visible=False), \"No changes applied\", gr.update(visible=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Math/Financial Functions\n",
        "# ---------------------------\n",
        "\n",
        "# Basic Arithmetic\n",
        "def add_series(series, operand): return series + operand\n",
        "def subtract_series(series, operand): return series - operand\n",
        "def multiply_series(series, operand): return series * operand\n",
        "\n",
        "def divide_series(series, operand, result_type='float'):\n",
        "    if isinstance(operand, pd.Series):\n",
        "        denom = operand.where(~np.isclose(operand, 0.0, atol=1e-12), np.nan)\n",
        "    else:\n",
        "        denom = np.nan if np.isclose(float(operand), 0.0, atol=1e-12) else operand\n",
        "    result = series / denom\n",
        "    rt = (result_type or 'float').lower()\n",
        "    if rt == 'percent':\n",
        "        result = result * 100.0\n",
        "    elif rt == 'int':\n",
        "        with np.errstate(invalid='ignore'):\n",
        "            result = np.floor(result).astype('Int64')\n",
        "    return result\n",
        "\n",
        "# Statistical\n",
        "def sum_series(series): return series.sum()\n",
        "\n",
        "def crossec(series, method='mean'):\n",
        "    method = (method or 'mean').lower()\n",
        "    if method == 'mean': return series.mean()\n",
        "    if method == 'sum': return series.sum()\n",
        "    if method == 'max': return series.max()\n",
        "    if method == 'min': return series.min()\n",
        "    if method == 'median': return series.median()\n",
        "    if method == 'std': return series.std()\n",
        "    return series.mean()\n",
        "\n",
        "def rollsum(series, period):\n",
        "    period = int(period)\n",
        "    if period <= 0: period = 1\n",
        "    return series.rolling(window=period, min_periods=period).sum()\n",
        "\n",
        "# Transformation\n",
        "def accumulate(series, method='sum'):\n",
        "    method = (method or 'sum').lower()\n",
        "    if method == 'sum':\n",
        "        return series.cumsum()\n",
        "    elif method == 'avg':\n",
        "        return series.expanding().mean()\n",
        "    else:\n",
        "        return series.cumsum()\n",
        "\n",
        "def antilog(series): return np.exp(series)\n",
        "def exp_series(series): return np.exp(series)\n",
        "\n",
        "def log_series(series):\n",
        "    s = series.copy()\n",
        "    s = s.where(s > 0, np.nan)\n",
        "    return np.log(s)\n",
        "\n",
        "def log10_series(series):\n",
        "    s = series.copy()\n",
        "    s = s.where(s > 0, np.nan)\n",
        "    return np.log10(s)\n",
        "\n",
        "def power_series(series, exponent): return np.power(series, exponent)\n",
        "def recip(series): return 1 / series.replace(0, np.nan)\n",
        "def round_series(series, decimals=0): return series.round(int(decimals))\n",
        "\n",
        "def sqrt_series(series):\n",
        "    s = series.copy()\n",
        "    s = s.where(s >= 0, np.nan)\n",
        "    return np.sqrt(s)\n",
        "\n",
        "# Time Series helpers\n",
        "def percentage_change(series, period=1):\n",
        "    return series.pct_change(periods=int(period)) * 100\n",
        "\n",
        "def exponential_growth(series, period=1):\n",
        "    # Same as percent change: rate of change over lag\n",
        "    return series.pct_change(periods=int(period)) * 100\n",
        "\n",
        "def compound_growth_rate(series, periods):\n",
        "    periods = int(periods)\n",
        "    s_clean = series.dropna()\n",
        "    if len(s_clean) < 2 or periods <= 0:\n",
        "        return np.nan\n",
        "    start = s_clean.iloc[0]\n",
        "    end = s_clean.iloc[-1]\n",
        "    if start <= 0:\n",
        "        return np.nan\n",
        "    return ((end / start) ** (1 / periods) - 1) * 100\n",
        "\n",
        "def difference(series, period=1):\n",
        "    return series.diff(periods=int(period))\n",
        "\n",
        "# Robust cleaner for resampling (handles duplicates safely)\n",
        "def clean_time_series_for_resample(values, time_index, dup_agg='mean'):\n",
        "    idx = pd.to_datetime(time_index, errors='coerce')\n",
        "    vals = pd.to_numeric(pd.Series(values), errors=\"coerce\")\n",
        "    s = pd.Series(vals.values, index=idx)\n",
        "    s = s[~s.index.isna()].sort_index()\n",
        "    if s.index.has_duplicates:\n",
        "        s = s.groupby(level=0).agg(dup_agg)\n",
        "    return s.dropna()\n",
        "\n",
        "def aggregate(series, time_index, frequency='M', method='sum', align_end=True):\n",
        "    if time_index is None:\n",
        "        raise ValueError(\"Please choose a time column (datetime) for aggregation.\")\n",
        "    s = clean_time_series_for_resample(series, time_index, dup_agg='mean')\n",
        "    label = 'right' if align_end else 'left'\n",
        "    closed = 'right' if align_end else 'left'\n",
        "    res = s.resample(frequency, label=label, closed=closed)\n",
        "    method = (method or 'sum').lower()\n",
        "    if method == 'sum': out = res.sum()\n",
        "    elif method == 'mean': out = res.mean()\n",
        "    elif method == 'max': out = res.max()\n",
        "    elif method == 'min': out = res.min()\n",
        "    elif method == 'first': out = res.first()\n",
        "    elif method == 'last': out = res.last()\n",
        "    else: out = res.sum()\n",
        "    return out\n",
        "\n",
        "def disaggregate(series, time_index, frequency='D', method='linear', align_end=True):\n",
        "    if time_index is None:\n",
        "        raise ValueError(\"Please choose a time column (datetime) for disaggregation.\")\n",
        "    s = clean_time_series_for_resample(series, time_index, dup_agg='mean')\n",
        "    label = 'right' if align_end else 'left'\n",
        "    closed = 'right' if align_end else 'left'\n",
        "    up = s.resample(frequency, label=label, closed=closed).asfreq()\n",
        "    m = (method or 'linear').lower()\n",
        "    if m == 'linear':\n",
        "        out = up.interpolate(method='time')\n",
        "    elif m == 'spline':\n",
        "        try:\n",
        "            out = up.interpolate(method='spline', order=3)\n",
        "        except Exception:\n",
        "            out = up.interpolate(method='time')\n",
        "    elif m == 'ffill':\n",
        "        out = up.ffill()\n",
        "    else:\n",
        "        out = up.interpolate(method='time')\n",
        "    return out\n",
        "\n",
        "# Conversion\n",
        "def unit_conversion(series, multiplier): return series * float(multiplier)\n",
        "def unit_multiplier(series, multiplier): return series * float(multiplier)\n",
        "def convert_currency(series, rate): return series * float(rate)\n",
        "def to_numeric_series(df, col): return robust_to_numeric(df[col])\n",
        "\n",
        "# ---------------------------\n",
        "# Deseasonalization helpers\n",
        "# ---------------------------\n",
        "def to_time_series(values, time_index):\n",
        "    idx = pd.to_datetime(time_index, errors='coerce')\n",
        "    vals = robust_to_numeric(values)\n",
        "    mask = (~pd.isna(idx)) & (~pd.isna(vals.values))\n",
        "    s = pd.Series(vals.values[mask], index=idx[mask]).sort_index()\n",
        "    s = s.groupby(level=0).mean()\n",
        "    return s\n",
        "\n",
        "def deseasonalize_series(values, time_index, period, model='multiplicative'):\n",
        "    if period is None or int(period) < 2:\n",
        "        raise ValueError(\"Seasonal period must be an integer >= 2.\")\n",
        "    period = int(period)\n",
        "    s = to_time_series(values, time_index)\n",
        "    m = (str(model) or 'multiplicative').lower()\n",
        "    if m.startswith('mult'):\n",
        "        if (s <= 0).any():\n",
        "            raise ValueError(\"Multiplicative deseasonalization requires positive values.\")\n",
        "        stl = STL(np.log(s), period=period, robust=True)\n",
        "        res = stl.fit()\n",
        "        deseason = np.exp(np.log(s) - res.seasonal)\n",
        "        deseason = pd.Series(deseason, index=s.index)\n",
        "    else:\n",
        "        stl = STL(s, period=period, robust=True)\n",
        "        res = stl.fit()\n",
        "        deseason = s - res.seasonal\n",
        "    return deseason\n",
        "\n",
        "def deseasonalize_with_cycle(values, time_index, period, model='multiplicative'):\n",
        "    if period is None or int(period) < 2:\n",
        "        raise ValueError(\"Seasonal period must be an integer >= 2.\")\n",
        "    period = int(period)\n",
        "    s = to_time_series(values, time_index)\n",
        "    m = (str(model) or 'multiplicative').lower()\n",
        "    if m.startswith('mult'):\n",
        "        if (s <= 0).any():\n",
        "            raise ValueError(\"Multiplicative deseasonalization requires strictly positive values.\")\n",
        "        stl = STL(np.log(s), period=period, robust=True)\n",
        "        res = stl.fit()\n",
        "        seasonal_series = np.exp(res.seasonal)\n",
        "        ds = s / seasonal_series\n",
        "        kind = 'factor'\n",
        "    else:\n",
        "        stl = STL(s, period=period, robust=True)\n",
        "        res = stl.fit()\n",
        "        seasonal_series = res.seasonal\n",
        "        ds = s - seasonal_series\n",
        "        kind = 'additive'\n",
        "    n = len(s)\n",
        "    positions = np.arange(n) % period\n",
        "    cycle = np.zeros(period, dtype=float)\n",
        "    for k in range(period):\n",
        "        vals = np.asarray(seasonal_series)[positions == k]\n",
        "        cycle[k] = float(np.mean(vals)) if len(vals) else (1.0 if kind == 'factor' else 0.0)\n",
        "    last_pos = (n - 1) % period\n",
        "    return ds, s, cycle, kind, last_pos\n",
        "\n",
        "def reseasonalize_from_cycle(ds_forecast, start_pos, cycle, kind):\n",
        "    n = len(ds_forecast)\n",
        "    period = len(cycle)\n",
        "    out = np.empty(n, dtype=float)\n",
        "    for i in range(n):\n",
        "        cyc_val = cycle[(start_pos + 1 + i) % period]\n",
        "        out[i] = ds_forecast[i] + cyc_val if kind == 'additive' else ds_forecast[i] * cyc_val\n",
        "    return out\n",
        "\n",
        "def make_ds_preview_fig(index, original, deseasonalized):\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=index, y=np.asarray(original), mode='lines+markers', name='Original Time Series',\n",
        "                             line=dict(color='#1f77b4', width=2), marker=dict(size=4, opacity=0.8)))\n",
        "    fig.add_trace(go.Scatter(x=index, y=np.asarray(deseasonalized), mode='lines+markers', name='Deseasonalized Time Series',\n",
        "                             line=dict(color='#2ca02c', width=2.5), marker=dict(size=4, opacity=0.8)))\n",
        "    fig.update_layout(\n",
        "        title=dict(text='Original vs. Deseasonalized Time Series', font=dict(size=18, color='#333')),\n",
        "        xaxis_title='Date', yaxis_title='Value', hovermode='x unified', legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
        "        template='plotly_white', margin=dict(l=40, r=40, t=60, b=40),\n",
        "        xaxis=dict(showgrid=True, gridcolor='#e0e0e0'), yaxis=dict(showgrid=True, gridcolor='#e0e0e0')\n",
        "    )\n",
        "    return fig\n",
        "\n",
        "def make_empty_note_fig(note_text):\n",
        "    fig = go.Figure()\n",
        "    fig.add_annotation(text=note_text, xref=\"paper\", yref=\"paper\", x=0.5, y=0.5, showarrow=False, font=dict(size=14, color='#666'))\n",
        "    fig.update_layout(xaxis=dict(visible=False), yaxis=dict(visible=False), title=dict(text=\"Deseasonalization Preview\", font=dict(size=16)))\n",
        "    return fig\n",
        "\n",
        "def detect_datetime_cols(df):\n",
        "    return [c for c in df.columns if pd.api.types.is_datetime64_any_dtype(df[c])]\n",
        "\n",
        "def get_series_with_time(df, column_name, time_column=None):\n",
        "    # Return series and associated time index if provided\n",
        "    if time_column and time_column in df.columns:\n",
        "        idx = pd.to_datetime(df[time_column], errors='coerce')\n",
        "    else:\n",
        "        tcols = detect_datetime_cols(df)\n",
        "        idx = pd.to_datetime(df[tcols[0]], errors='coerce') if tcols else None\n",
        "\n",
        "    s = robust_to_numeric(df[column_name])\n",
        "    if idx is not None:\n",
        "        tmp = pd.DataFrame({\"_val\": s, \"_t\": idx}).dropna(subset=[\"_val\", \"_t\"]).sort_values(\"_t\")\n",
        "        return tmp[\"_val\"].reset_index(drop=True), tmp[\"_t\"].reset_index(drop=True)\n",
        "    else:\n",
        "        # No time column – return None for time to avoid fake dates downstream\n",
        "        return s.reset_index(drop=True), None\n",
        "\n",
        "def format_date_series_for_output(dt_index_like):\n",
        "    try:\n",
        "        if isinstance(dt_index_like, pd.DatetimeIndex):\n",
        "            return dt_index_like.strftime('%Y-%m-%d').tolist()\n",
        "        if isinstance(dt_index_like, pd.PeriodIndex):\n",
        "            return dt_index_like.to_timestamp().strftime('%Y-%m-%d').tolist()\n",
        "        s = pd.Series(dt_index_like)\n",
        "        dt = pd.to_datetime(s, errors='coerce')\n",
        "        if dt.dropna().empty:\n",
        "            return s.astype(str).tolist()\n",
        "        return dt.dt.strftime('%Y-%m-%d').tolist()\n",
        "    except Exception:\n",
        "        try:\n",
        "            return pd.Series(dt_index_like).astype(str).tolist()\n",
        "        except Exception:\n",
        "            return dt_index_like\n",
        "\n",
        "# ---------------------------\n",
        "# Math Application\n",
        "# ---------------------------\n",
        "def apply_math_function(\n",
        "    selected_function, column_name,\n",
        "    operand_source, operand_value, operand_column,\n",
        "    numeric_param, method_param, frequency_param,\n",
        "    result_type, time_column, align_end,\n",
        "    ds_use_in_math, ds_math_period, ds_math_model,\n",
        "    add_to_data, result_col_name, show_overlay\n",
        "):\n",
        "    global current_filtered_df, tables_data\n",
        "\n",
        "    def _dropdown_updates_for_current_df(new_col_preselect=None):\n",
        "        numeric_cols = current_filtered_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        time_cols = detect_datetime_cols(current_filtered_df)\n",
        "        arima_col_upd = gr.update(choices=numeric_cols, value=(new_col_preselect if (new_col_preselect in numeric_cols) else (numeric_cols[0] if numeric_cols else None)))\n",
        "        arima_time_upd = gr.update(choices=time_cols, value=(time_cols[0] if time_cols else None))\n",
        "        math_col_upd = gr.update(choices=numeric_cols, value=(new_col_preselect if (new_col_preselect in numeric_cols) else (numeric_cols[0] if numeric_cols else None)))\n",
        "        table_sel_upd = gr.update(choices=list(tables_data.keys()), value=(\"Transformed Table\" if \"Transformed Table\" in tables_data else (list(tables_data.keys())[0] if tables_data else None)))\n",
        "        table_prev_df = tables_data.get(\"Transformed Table\", current_filtered_df)\n",
        "        return arima_col_upd, arima_time_upd, math_col_upd, table_sel_upd, table_prev_df\n",
        "\n",
        "    if current_filtered_df.empty or column_name not in current_filtered_df.columns:\n",
        "        ar_col, ar_time, m_col, t_sel, t_prev = _dropdown_updates_for_current_df()\n",
        "        return None, \"Error: No data or column not found\", gr.update(visible=False), gr.update(visible=False), go.Figure(), ar_col, ar_time, m_col, t_sel, t_prev\n",
        "\n",
        "    fn = (selected_function or \"\").upper()\n",
        "    user_col = (result_col_name or \"\").strip()\n",
        "    final_col_name = user_col if user_col else (\"Deseasonalized\" if fn == \"DESEASONALIZE\" else f\"{fn}_{column_name}\")\n",
        "\n",
        "    base_series, base_time = get_series_with_time(current_filtered_df, column_name, time_column)\n",
        "    series = pd.Series(base_series)\n",
        "\n",
        "    have_time = (time_column is not None and time_column in current_filtered_df.columns and base_time is not None)\n",
        "    orig_ts = clean_time_series_for_resample(series, base_time, dup_agg='mean') if have_time else None\n",
        "\n",
        "    # Use deseasonalized input optionally (except when running the DESEASONALIZE function itself)\n",
        "    use_ds = bool(ds_use_in_math) and (fn != \"DESEASONALIZE\")\n",
        "    in_ts = orig_ts\n",
        "    if use_ds:\n",
        "        if not have_time:\n",
        "            ar_col, ar_time, m_col, t_sel, t_prev = _dropdown_updates_for_current_df()\n",
        "            return None, \"Error: Deseasonalized input requires a Time Column.\", gr.update(visible=False), gr.update(visible=False), go.Figure(), ar_col, ar_time, m_col, t_sel, t_prev\n",
        "        if ds_math_period is None or int(ds_math_period) < 2:\n",
        "            ar_col, ar_time, m_col, t_sel, t_prev = _dropdown_updates_for_current_df()\n",
        "            return None, \"Error: DS Seasonal Period must be >= 2.\", gr.update(visible=False), gr.update(visible=False), go.Figure(), ar_col, ar_time, m_col, t_sel, t_prev\n",
        "        ds_model = ds_math_model or 'multiplicative'\n",
        "        # Compute deseasonalized time series based on original time series\n",
        "        try:\n",
        "            ds_ts = deseasonalize_series(orig_ts.values, orig_ts.index, int(ds_math_period), model=ds_model)\n",
        "        except Exception as de:\n",
        "            ar_col, ar_time, m_col, t_sel, t_prev = _dropdown_updates_for_current_df()\n",
        "            return None, f\"Error applying deseasonalized input: {de}\", gr.update(visible=False), gr.update(visible=False), go.Figure(), ar_col, ar_time, m_col, t_sel, t_prev\n",
        "        in_ts = ds_ts\n",
        "        series = ds_ts.copy()  # for non-time functions\n",
        "\n",
        "    # Clean, time-ordered series for time-dependent functions\n",
        "    time_funcs = {\"ROLLSUM\", \"%CHANGE\", \"%EXPGROWTH\", \"CAGR\", \"DIFF\", \"ACCUMULATE\"}\n",
        "    ts = in_ts if (fn in time_funcs) else None\n",
        "\n",
        "    def get_operand():\n",
        "        if fn in [\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"]:\n",
        "            src = (operand_source or 'value').lower()\n",
        "            if src == 'column':\n",
        "                if not operand_column or operand_column not in current_filtered_df.columns:\n",
        "                    raise ValueError(\"Choose a valid operand column.\")\n",
        "                op_vals, op_time = get_series_with_time(current_filtered_df, operand_column, time_column if have_time else None)\n",
        "                if have_time:\n",
        "                    op_ts = clean_time_series_for_resample(op_vals, op_time, dup_agg='mean')\n",
        "                    # Align to the base input (deseasonalized if enabled)\n",
        "                    base_align = in_ts if in_ts is not None else orig_ts\n",
        "                    return op_ts.reindex(base_align.index)\n",
        "                else:\n",
        "                    return pd.Series(op_vals).reset_index(drop=True).reindex_like(series)\n",
        "            else:\n",
        "                if operand_value is None:\n",
        "                    raise ValueError(\"Provide a numeric value for the operation.\")\n",
        "                return float(operand_value)\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        result = None\n",
        "\n",
        "        if fn in [\"ACCUMULATE\", \"ANTILOG\", \"EXP\", \"LOG\", \"LOG10\", \"RECIP\", \"SQRT\", \"SUM\", \"CROSSSEC\"]:\n",
        "            if fn == \"ACCUMULATE\":\n",
        "                if ts is None:\n",
        "                    raise ValueError(\"ACCUMULATE requires a Time Column.\")\n",
        "                result = accumulate(ts, method_param or 'sum')\n",
        "            elif fn == \"ANTILOG\": result = antilog(series)\n",
        "            elif fn == \"EXP\": result = exp_series(series)\n",
        "            elif fn == \"LOG\": result = log_series(series)\n",
        "            elif fn == \"LOG10\": result = log10_series(series)\n",
        "            elif fn == \"RECIP\": result = recip(series)\n",
        "            elif fn == \"SQRT\": result = sqrt_series(series)\n",
        "            elif fn == \"SUM\": result = sum_series(series)\n",
        "            elif fn == \"CROSSSEC\": result = crossec(series, method_param or 'mean')\n",
        "\n",
        "        elif fn in [\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"]:\n",
        "            operand = get_operand()\n",
        "            base_in = in_ts if have_time else series\n",
        "            if isinstance(operand, pd.Series):\n",
        "                if fn == \"ADD\": result = add_series(base_in, operand)\n",
        "                elif fn == \"SUBTRACT\": result = subtract_series(base_in, operand)\n",
        "                elif fn == \"MULTIPLY\": result = multiply_series(base_in, operand)\n",
        "                elif fn == \"DIVIDE\": result = divide_series(base_in, operand, result_type)\n",
        "            else:\n",
        "                if fn == \"ADD\": result = add_series(base_in, float(operand))\n",
        "                elif fn == \"SUBTRACT\": result = subtract_series(base_in, float(operand))\n",
        "                elif fn == \"MULTIPLY\": result = multiply_series(base_in, float(operand))\n",
        "                elif fn == \"DIVIDE\": result = divide_series(base_in, float(operand), result_type)\n",
        "\n",
        "        elif fn == \"POWER\":\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Exponent is required for POWER function.\")\n",
        "            result = power_series(series, float(numeric_param))\n",
        "\n",
        "        elif fn == \"ROLLSUM\":\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Window size is required for ROLLSUM.\")\n",
        "            if ts is None:\n",
        "                raise ValueError(\"ROLLSUM requires a Time Column.\")\n",
        "            result = rollsum(ts, int(numeric_param))\n",
        "\n",
        "        elif fn == \"ROUND\":\n",
        "            result = round_series(series, int(numeric_param if numeric_param is not None else 0))\n",
        "\n",
        "        elif fn == \"%CHANGE\":\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Period is required for %CHANGE function.\")\n",
        "            if ts is None:\n",
        "                raise ValueError(\"%CHANGE requires a Time Column.\")\n",
        "            result = percentage_change(ts, int(numeric_param))\n",
        "\n",
        "        elif fn == \"%EXPGROWTH\":\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Period is required for %EXPGROWTH function.\")\n",
        "            if ts is None:\n",
        "                raise ValueError(\"%EXPGROWTH requires a Time Column.\")\n",
        "            result = exponential_growth(ts, int(numeric_param))\n",
        "\n",
        "        elif fn == \"CAGR\":\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Compounding periods are required for CAGR function.\")\n",
        "            if ts is None:\n",
        "                raise ValueError(\"CAGR requires a Time Column.\")\n",
        "            result = compound_growth_rate(ts, int(numeric_param))\n",
        "\n",
        "        elif fn == \"DIFF\":\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Period is required for DIFF function.\")\n",
        "            if ts is None:\n",
        "                raise ValueError(\"DIFF requires a Time Column.\")\n",
        "            result = difference(ts, int(numeric_param))\n",
        "\n",
        "        elif fn in [\"UNIT_CONVERSION\", \"UNIT_MULTIPLIER\", \"CONVERTCUR\"]:\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(f\"Multiplier/rate is required for {fn} function.\")\n",
        "            factor = float(numeric_param)\n",
        "            if fn == \"UNIT_CONVERSION\": result = unit_conversion(series, factor)\n",
        "            elif fn == \"UNIT_MULTIPLIER\": result = unit_multiplier(series, factor)\n",
        "            else: result = convert_currency(series, factor)\n",
        "\n",
        "        elif fn == \"AGGREGATE\":\n",
        "            if time_column is None or time_column not in current_filtered_df.columns:\n",
        "                raise ValueError(\"Please choose a Time Column (datetime) for AGGREGATE.\")\n",
        "            freq = frequency_param or 'M'\n",
        "            if use_ds and in_ts is not None:\n",
        "                result = aggregate(in_ts.values, in_ts.index, frequency=freq, method=method_param or 'sum', align_end=bool(align_end))\n",
        "            else:\n",
        "                result = aggregate(to_numeric_series(current_filtered_df, column_name),\n",
        "                                   current_filtered_df[time_column],\n",
        "                                   frequency=freq, method=method_param or 'sum', align_end=bool(align_end))\n",
        "        elif fn == \"DISAGGREGATE\":\n",
        "            if time_column is None or time_column not in current_filtered_df.columns:\n",
        "                raise ValueError(\"Please choose a Time Column (datetime) for DISAGGREGATE.\")\n",
        "            freq = frequency_param or 'D'\n",
        "            if use_ds and in_ts is not None:\n",
        "                result = disaggregate(in_ts.values, in_ts.index, frequency=freq, method=method_param or 'linear', align_end=bool(align_end))\n",
        "            else:\n",
        "                result = disaggregate(to_numeric_series(current_filtered_df, column_name),\n",
        "                                      current_filtered_df[time_column],\n",
        "                                      frequency=freq, method=method_param or 'linear', align_end=bool(align_end))\n",
        "        elif fn == \"DESEASONALIZE\":\n",
        "            if time_column is None or time_column not in current_filtered_df.columns:\n",
        "                raise ValueError(\"Please choose a Time Column for Deseasonalization.\")\n",
        "            if numeric_param is None:\n",
        "                raise ValueError(\"Seasonal period is required for Deseasonalization.\")\n",
        "            model = method_param or 'multiplicative'\n",
        "            result = deseasonalize_series(to_numeric_series(current_filtered_df, column_name),\n",
        "                                          current_filtered_df[time_column], int(numeric_param), model=model)\n",
        "        else:\n",
        "            ar_col, ar_time, m_col, t_sel, t_prev = _dropdown_updates_for_current_df()\n",
        "            return None, \"Error: Function not implemented\", gr.update(visible=False), gr.update(visible=False), go.Figure(), ar_col, ar_time, m_col, t_sel, t_prev\n",
        "\n",
        "        scalar_output = np.isscalar(result)\n",
        "\n",
        "        # Build output df\n",
        "        if scalar_output:\n",
        "            out_df = pd.DataFrame({\"Function\": [fn], \"Column\": [column_name], \"Result\": [result]})\n",
        "        else:\n",
        "            if isinstance(result, pd.Series) and isinstance(result.index, pd.DatetimeIndex):\n",
        "                ser = result\n",
        "                out_df = pd.DataFrame({\"Date\": format_date_series_for_output(ser.index), final_col_name: ser.values})\n",
        "            else:\n",
        "                ser = pd.Series(result).reset_index(drop=True)\n",
        "                if base_time is not None and len(ser) == len(pd.Series(base_series)):\n",
        "                    out_df = pd.DataFrame({final_col_name: ser.values})\n",
        "                else:\n",
        "                    out_df = pd.DataFrame({final_col_name: ser.values})\n",
        "\n",
        "        # Save download\n",
        "        meta = get_filter_suffix_for_filename()\n",
        "        out_save = maybe_attach_static_cols(out_df, meta)\n",
        "        math_path = make_temp_csv(out_save, f\"math_{fn.lower()}\", meta)\n",
        "\n",
        "        cur_download_update = gr.update(visible=False)\n",
        "        new_col_name = None\n",
        "        status_msg = f\"Success: {fn}\" + (\" (using deseasonalized input)\" if use_ds else \"\")\n",
        "\n",
        "        if not scalar_output and add_to_data:\n",
        "            new_col_name = final_col_name\n",
        "            if isinstance(result, pd.Series) and isinstance(result.index, pd.DatetimeIndex) and fn in [\"AGGREGATE\", \"DISAGGREGATE\"]:\n",
        "                time_name = time_column or \"Period\"\n",
        "                transformed_df = out_df.rename(columns={\"Date\": time_name})\n",
        "                if \"Date\" in transformed_df.columns:\n",
        "                    transformed_df[time_name] = pd.to_datetime(transformed_df[time_name], errors=\"coerce\")\n",
        "                    transformed_df = transformed_df.drop(columns=[\"Date\"])\n",
        "                current_filtered_df = transformed_df.copy()\n",
        "                tables_data[\"Transformed Table\"] = current_filtered_df.copy()\n",
        "                status_msg = f\"Success: {fn}\" + (\" (using deseasonalized input)\" if use_ds else \"\") + f\" | Created 'Transformed Table' with [{time_name}, {new_col_name}] and set it as current data.\"\n",
        "            else:\n",
        "                if isinstance(result, pd.Series) and isinstance(result.index, pd.DatetimeIndex) and have_time and time_column in current_filtered_df.columns:\n",
        "                    temp = pd.DataFrame({time_column: result.index, new_col_name: result.values})\n",
        "                    temp[time_column] = pd.to_datetime(temp[time_column], errors=\"coerce\")\n",
        "                    current_filtered_df[time_column] = pd.to_datetime(current_filtered_df[time_column], errors=\"coerce\")\n",
        "                    current_filtered_df = current_filtered_df.merge(temp[[time_column, new_col_name]], on=time_column, how=\"left\")\n",
        "                    status_msg = f\"Success: {fn}\" + (\" (using deseasonalized input)\" if use_ds else \"\") + f\" | Added column '{new_col_name}' aligned by time.\"\n",
        "                elif len(pd.Series(result)) != len(current_filtered_df):\n",
        "                    status_msg = f\"Success: {fn}\" + (\" (using deseasonalized input)\" if use_ds else \"\") + \" | Result length differs from data; not added.\"\n",
        "                else:\n",
        "                    current_filtered_df[new_col_name] = pd.Series(result).values\n",
        "                    status_msg = f\"Success: {fn}\" + (\" (using deseasonalized input)\" if use_ds else \"\") + f\" | Added column '{new_col_name}' to current data.\"\n",
        "\n",
        "                tables_data[\"Transformed Table\"] = current_filtered_df.copy()\n",
        "\n",
        "            cur_df = maybe_attach_static_cols(current_filtered_df, meta)\n",
        "            cur_path = make_temp_csv(cur_df, \"current_data\", meta)\n",
        "            cur_download_update = gr.update(value=cur_path, visible=True)\n",
        "\n",
        "        # Visualization (two separate charts when overlay is enabled)\n",
        "        result_is_percent = (fn in [\"%CHANGE\", \"%EXPGROWTH\"]) or (fn == \"DIVIDE\" and (result_type or \"\").lower() == \"percent\")\n",
        "\n",
        "        if scalar_output:\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(go.Indicator(mode=\"number\", value=result, title=dict(text=fn, font=dict(size=18))))\n",
        "            fig.update_layout(title=dict(text=f\"Result of {fn}\", font=dict(size=20, color='#333')), template='plotly_white')\n",
        "        else:\n",
        "            if isinstance(result, pd.Series) and isinstance(result.index, pd.DatetimeIndex):\n",
        "                x_trans = result.index\n",
        "                y_trans = result.values\n",
        "            else:\n",
        "                ser = pd.Series(result).reset_index(drop=True)\n",
        "                if have_time and in_ts is not None:\n",
        "                    x_trans = np.arange(len(ser)) if len(ser) != len(in_ts) else in_ts.index\n",
        "                else:\n",
        "                    x_trans = np.arange(len(ser))\n",
        "                y_trans = ser.values\n",
        "\n",
        "            # Original series for overlay (always original, not deseasonalized)\n",
        "            if have_time and orig_ts is not None:\n",
        "                x_orig = orig_ts.index\n",
        "                y_original = orig_ts.values\n",
        "            else:\n",
        "                x_orig = np.arange(len(series))\n",
        "                y_original = pd.Series(base_series).values\n",
        "\n",
        "            if show_overlay:\n",
        "                fig = make_subplots(\n",
        "                    rows=2, cols=1, shared_xaxes=False, vertical_spacing=0.08,\n",
        "                    subplot_titles=(\"Original Series\", f\"{final_col_name}\" + (\" (%)\" if result_is_percent else \"\"))\n",
        "                )\n",
        "                fig.add_trace(go.Scatter(x=x_orig, y=y_original, mode='lines+markers', name='Original Series', line=dict(color='#1f77b4', width=2), marker=dict(size=4, opacity=0.8)), row=1, col=1)\n",
        "                fig.add_trace(go.Scatter(x=x_trans, y=y_trans, mode='lines+markers', name=final_col_name, line=dict(color='#d62728', width=2), marker=dict(size=4, opacity=0.8)), row=2, col=1)\n",
        "                fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
        "                fig.update_yaxes(title_text=(\"Percent (%)\" if result_is_percent else \"Value\"), row=2, col=1)\n",
        "                fig.update_xaxes(title_text=\"Date/Index\", title_standoff=8, row=1, col=1)\n",
        "                fig.update_xaxes(title_text=\"Date/Index\", title_standoff=8, row=2, col=1)\n",
        "                fig.update_layout(title=dict(text=f\"Original (top) and {final_col_name} (bottom)\", font=dict(size=18, color='#333')), hovermode='x unified', legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1), template='plotly_white')\n",
        "            else:\n",
        "                fig = go.Figure()\n",
        "                fig.add_trace(go.Scatter(x=x_trans, y=y_trans, mode='lines+markers', name=final_col_name, line=dict(color='#d62728', width=2), marker=dict(size=4, opacity=0.8)))\n",
        "                y_title = 'Percent (%)' if result_is_percent else 'Value'\n",
        "                fig.update_layout(title=dict(text=f\"Transformed Series using {fn}\", font=dict(size=18, color='#333')), xaxis_title=\"Date/Index\", yaxis_title=y_title, hovermode='x unified', legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1), template='plotly_white')\n",
        "\n",
        "        ar_col_upd, ar_time_upd, math_col_upd, table_sel_upd, table_prev_df = _dropdown_updates_for_current_df(new_col_preselect=new_col_name)\n",
        "\n",
        "        return out_df, status_msg, gr.update(value=math_path, visible=True), cur_download_update, fig, \\\n",
        "               ar_col_upd, ar_time_upd, math_col_upd, table_sel_upd, table_prev_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Math Error: {e}\")\n",
        "        ar_col_upd, ar_time_upd, math_col_upd, table_sel_upd, table_prev_df = _dropdown_updates_for_current_df()\n",
        "        return None, f\"Error in {selected_function}: {str(e)}\", gr.update(visible=False), gr.update(visible=False), go.Figure(), \\\n",
        "               ar_col_upd, ar_time_upd, math_col_upd, table_sel_upd, table_prev_df\n",
        "\n",
        "# ---------------------------\n",
        "# ARIMA helpers (updated DS options)\n",
        "# ---------------------------\n",
        "def calculate_smape(actual, forecast):\n",
        "    actual = np.array(actual)\n",
        "    forecast = np.array(forecast)\n",
        "    denom = np.abs(actual) + np.abs(forecast)\n",
        "    denom = np.where(denom == 0, 1e-10, denom)\n",
        "    smape_values = 2.0 * np.abs(forecast - actual) / denom\n",
        "    smape_values = smape_values[np.isfinite(smape_values)]\n",
        "    if len(smape_values) == 0:\n",
        "        return 0.0\n",
        "    return np.mean(smape_values) * 100\n",
        "\n",
        "def run_arima(column_name, periods=5, p=1, d=1, q=1, time_col=None, freq_choice='infer',\n",
        "              overlay_deseasonalize=True, ds_period=12, ds_model='multiplicative',\n",
        "              use_ds_for_arima=False, reseasonalize_output=True):\n",
        "    global current_filtered_df\n",
        "    if current_filtered_df.empty or column_name not in current_filtered_df.columns:\n",
        "        return None, None, \"Please load data and select a valid numeric column.\", make_empty_note_fig(\"No data\"), gr.update(visible=False), gr.update(visible=False)\n",
        "    if not time_col or time_col not in current_filtered_df.columns or not pd.api.types.is_datetime64_any_dtype(current_filtered_df[time_col]):\n",
        "        return None, None, \"Please select a time column (datetime) for ARIMA.\", make_empty_note_fig(\"Select time column\"), gr.update(visible=False), gr.update(visible=False)\n",
        "    try:\n",
        "        df2 = current_filtered_df[[time_col, column_name]].copy()\n",
        "        df2[time_col] = pd.to_datetime(df2[time_col], errors='coerce')\n",
        "        df2[column_name] = robust_to_numeric(df2[column_name])\n",
        "        df2 = df2.dropna(subset=[time_col, column_name]).sort_values(time_col)\n",
        "        s = df2.groupby(time_col)[column_name].mean()\n",
        "        if len(s) < 10:\n",
        "            raise ValueError(\"Insufficient data points for ARIMA (min: 10 after cleaning).\")\n",
        "\n",
        "        # Frequency inference\n",
        "        if freq_choice and str(freq_choice).lower() != \"infer\":\n",
        "            freq_to_use = str(freq_choice)\n",
        "        else:\n",
        "            try:\n",
        "                freq_to_use = pd.infer_freq(s.index)\n",
        "            except Exception:\n",
        "                freq_to_use = None\n",
        "\n",
        "        overlay_msg = \"\"\n",
        "        ds_preview_fig = make_empty_note_fig(\"Enable overlay or DS-input to preview deseasonalization\")\n",
        "\n",
        "        ds_for_overlay = None\n",
        "        ds_series_download_update = gr.update(visible=False)\n",
        "        # Overlay DS preview (separate from using DS for ARIMA)\n",
        "        if overlay_deseasonalize:\n",
        "            try:\n",
        "                ds_for_overlay = deseasonalize_series(s.values, s.index, int(ds_period or 12), model=ds_model)\n",
        "                ds_preview_fig = make_ds_preview_fig(ds_for_overlay.index, s.loc[ds_for_overlay.index].values, ds_for_overlay.values)\n",
        "                ds_df = pd.DataFrame({\"Period\": format_date_series_for_output(ds_for_overlay.index), \"Deseasonalized\": ds_for_overlay.values})\n",
        "                meta = get_filter_suffix_for_filename()\n",
        "                ds_df_save = maybe_attach_static_cols(ds_df, meta)\n",
        "                ds_path = make_temp_csv(ds_df_save, \"deseasonalized_series\", meta)\n",
        "                ds_series_download_update = gr.update(value=ds_path, visible=True)\n",
        "            except Exception as de:\n",
        "                overlay_msg = f\" | Deseasonalization overlay failed: {de}\"\n",
        "                ds_preview_fig = make_empty_note_fig(f\"Deseasonalization failed: {de}\")\n",
        "                ds_series_download_update = gr.update(visible=False)\n",
        "\n",
        "        if use_ds_for_arima:\n",
        "            effective_model = ds_model\n",
        "            try:\n",
        "                ds_series, s_aligned, cycle, kind, last_pos = deseasonalize_with_cycle(\n",
        "                    s.values, s.index, int(ds_period or 12), model=ds_model\n",
        "                )\n",
        "            except Exception as e:\n",
        "                ds_series, s_aligned, cycle, kind, last_pos = deseasonalize_with_cycle(\n",
        "                    s.values, s.index, int(ds_period or 12), model='additive'\n",
        "                )\n",
        "                effective_model = 'additive'\n",
        "                overlay_msg += f\" | Multiplicative DS not possible: {e}. Fallback to additive.\"\n",
        "\n",
        "            if ds_for_overlay is None:\n",
        "                ds_preview_fig = make_ds_preview_fig(s_aligned.index, s_aligned.values, ds_series.values)\n",
        "                ds_df2 = pd.DataFrame({\"Period\": format_date_series_for_output(ds_series.index), \"Deseasonalized\": ds_series.values})\n",
        "                meta = get_filter_suffix_for_filename()\n",
        "                ds_series_download_update = gr.update(value=make_temp_csv(maybe_attach_static_cols(ds_df2, meta), \"deseasonalized_series\", meta), visible=True)\n",
        "\n",
        "            ds_vals = ds_series.values\n",
        "            orig_vals = s_aligned.values\n",
        "            n = len(ds_vals)\n",
        "            split_point = max(1, int(n * 0.8))\n",
        "            train_ds, test_ds = ds_vals[:split_point], ds_vals[split_point:]\n",
        "            test_orig = orig_vals[split_point:]\n",
        "\n",
        "            model_v = ARIMA(train_ds, order=(int(p), int(d), int(q))).fit()\n",
        "            fc_val_ds = model_v.forecast(steps=len(test_ds))\n",
        "\n",
        "            # Metrics: re-seasonalize if requested; else evaluate in DS domain\n",
        "            if reseasonalize_output:\n",
        "                val_pred = reseasonalize_from_cycle(\n",
        "                    np.asarray(fc_val_ds), start_pos=(split_point - 1) % int(ds_period or 12),\n",
        "                    cycle=cycle, kind=('additive' if effective_model == 'additive' else 'factor')\n",
        "                )\n",
        "                mae = mean_absolute_error(test_orig, val_pred) if len(test_orig) > 0 else np.nan\n",
        "                rmse = np.sqrt(mean_squared_error(test_orig, val_pred)) if len(test_orig) > 0 else np.nan\n",
        "                smape = calculate_smape(test_orig, val_pred) if len(test_orig) > 0 else np.nan\n",
        "                metrics_text = (\n",
        "                    f\"Validation (20% holdout) — ARIMA on deseasonalized ({effective_model}); output re-seasonalized:\\n\"\n",
        "                    f\"Model: ARIMA({int(p)},{int(d)},{int(q)})\\n\"\n",
        "                    f\"MAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\nsMAPE: {smape:.2f}%{overlay_msg}\"\n",
        "                )\n",
        "            else:\n",
        "                mae = mean_absolute_error(test_ds, fc_val_ds) if len(test_ds) > 0 else np.nan\n",
        "                rmse = np.sqrt(mean_squared_error(test_ds, fc_val_ds)) if len(test_ds) > 0 else np.nan\n",
        "                smape = calculate_smape(test_ds, fc_val_ds) if len(test_ds) > 0 else np.nan\n",
        "                metrics_text = (\n",
        "                    f\"Validation (20% holdout) — ARIMA on deseasonalized ({effective_model}); output in DS units:\\n\"\n",
        "                    f\"Model: ARIMA({int(p)},{int(d)},{int(q)})\\n\"\n",
        "                    f\"MAE (DS): {mae:.4f}\\nRMSE (DS): {rmse:.4f}\\nsMAPE (DS): {smape:.2f}%{overlay_msg}\"\n",
        "                )\n",
        "\n",
        "            model_full = ARIMA(ds_vals, order=(int(p), int(d), int(q))).fit()\n",
        "            fc_ds_future = model_full.forecast(steps=int(periods))\n",
        "\n",
        "            last_date = s.index[-1]\n",
        "            if freq_to_use:\n",
        "                offset = pd.tseries.frequencies.to_offset(freq_to_use)\n",
        "                future_index = pd.date_range(start=last_date + offset, periods=int(periods), freq=offset)\n",
        "            else:\n",
        "                diffs = s.index.to_series().diff().dropna()\n",
        "                delta_td = pd.to_timedelta(diffs.mode().iloc[0]) if len(diffs) > 0 else pd.Timedelta(days=1)\n",
        "                future_index = pd.date_range(start=last_date + delta_td, periods=int(periods), freq=delta_td)\n",
        "\n",
        "            if reseasonalize_output:\n",
        "                reseason_future = reseasonalize_from_cycle(\n",
        "                    np.asarray(fc_ds_future),\n",
        "                    start_pos=last_pos, cycle=cycle,\n",
        "                    kind=('additive' if effective_model == 'additive' else 'factor')\n",
        "                )\n",
        "                forecast_df = pd.DataFrame({'Period': future_index, 'Forecast Value': reseason_future})\n",
        "            else:\n",
        "                forecast_df = pd.DataFrame({'Period': future_index, 'Forecast Value (DS)': np.asarray(fc_ds_future)})\n",
        "\n",
        "            history_df = pd.DataFrame({'Period': s.index, 'Value': s.values})\n",
        "\n",
        "            fig = go.Figure()\n",
        "            fig.add_trace(go.Scatter(x=history_df['Period'], y=history_df['Value'],\n",
        "                                     mode='lines+markers', name='Original Time Series',\n",
        "                                     line=dict(color='#1f77b4', width=2), marker=dict(color='#1f77b4', size=4, opacity=0.8)))\n",
        "            ds_plot_series = ds_for_overlay if ds_for_overlay is not None else ds_series\n",
        "            fig.add_trace(go.Scatter(x=ds_plot_series.index, y=ds_plot_series.values,\n",
        "                                     mode='lines+markers', name='Deseasonalized Time Series',\n",
        "                                     line=dict(color='#2ca02c', width=2.5), marker=dict(size=4, opacity=0.8)))\n",
        "            if reseasonalize_output:\n",
        "                fig.add_trace(go.Scatter(x=forecast_df['Period'], y=forecast_df['Forecast Value'],\n",
        "                                         mode='lines+markers', name='Forecast (reseasonalized)',\n",
        "                                         line=dict(color='#d62728', dash='dot', width=2), marker=dict(color='#d62728', size=4, opacity=0.8)))\n",
        "                y_title = 'Value'\n",
        "            else:\n",
        "                fig.add_trace(go.Scatter(x=forecast_df['Period'], y=forecast_df['Forecast Value (DS)'],\n",
        "                                         mode='lines+markers', name='Forecast (DS)',\n",
        "                                         line=dict(color='#d62728', dash='dot', width=2), marker=dict(color='#d62728', size=4, opacity=0.8)))\n",
        "                y_title = 'Value (DS)'\n",
        "\n",
        "            fig.update_layout(\n",
        "                title=dict(text=f'ARIMA — DS input={bool(use_ds_for_arima)}, Reseason={bool(reseasonalize_output)} — ARIMA({int(p)},{int(d)},{int(q)}) for {column_name}', font=dict(size=18, color='#333')),\n",
        "                xaxis_title='Date', yaxis_title=y_title, hovermode='x unified', legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
        "                template='plotly_white', margin=dict(l=40, r=40, t=60, b=40),\n",
        "                xaxis=dict(showgrid=True, gridcolor='#e0e0e0'), yaxis=dict(showgrid=True, gridcolor='#e0e0e0')\n",
        "            )\n",
        "\n",
        "            forecast_df_display = forecast_df.copy()\n",
        "            forecast_df_display['Period'] = format_date_series_for_output(forecast_df_display['Period'])\n",
        "            meta = get_filter_suffix_for_filename()\n",
        "            forecast_df_display = maybe_attach_static_cols(forecast_df_display, meta)\n",
        "            fc_path = make_temp_csv(forecast_df_display, \"arima_forecast\", meta)\n",
        "            return forecast_df_display, fig, metrics_text, ds_preview_fig, gr.update(value=fc_path, visible=True), ds_series_download_update\n",
        "\n",
        "        # Standard ARIMA on raw series\n",
        "        values = s.values\n",
        "        split_point = max(1, int(len(values) * 0.8))\n",
        "        model_val = ARIMA(train := values[:split_point], order=(int(p), int(d), int(q))).fit()\n",
        "        forecast_val = model_val.forecast(steps=len(values[split_point:]))\n",
        "        mae = mean_absolute_error(values[split_point:], forecast_val) if len(values[split_point:]) > 0 else np.nan\n",
        "        rmse = np.sqrt(mean_squared_error(values[split_point:], forecast_val)) if len(values[split_point:]) > 0 else np.nan\n",
        "        smape = calculate_smape(values[split_point:], forecast_val) if len(values[split_point:]) > 0 else np.nan\n",
        "        metrics_text = (\n",
        "            f\"Validation Metrics (20% holdout):\\n\"\n",
        "            f\"Model: ARIMA({int(p)},{int(d)},{int(q)})\\n\"\n",
        "            f\"MAE: {mae:.4f}\\nRMSE: {rmse:.4f}\\nsMAPE: {smape:.2f}%\"\n",
        "        )\n",
        "\n",
        "        model = ARIMA(values, order=(int(p), int(d), int(q))).fit()\n",
        "        fc_values = model.forecast(steps=int(periods))\n",
        "\n",
        "        if freq_to_use:\n",
        "            offset = pd.tseries.frequencies.to_offset(freq_to_use)\n",
        "            future_index = pd.date_range(start=s.index[-1] + offset, periods=int(periods), freq=offset)\n",
        "        else:\n",
        "            diffs = s.index.to_series().diff().dropna()\n",
        "            delta_td = pd.to_timedelta(diffs.mode().iloc[0]) if len(diffs) > 0 else pd.Timedelta(days=1)\n",
        "            future_index = pd.date_range(start=s.index[-1] + delta_td, periods=int(periods), freq=delta_td)\n",
        "\n",
        "        history_df = pd.DataFrame({'Period': s.index, 'Value': s.values})\n",
        "        forecast_df = pd.DataFrame({'Period': future_index, 'Forecast Value': np.asarray(fc_values)})\n",
        "\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(x=history_df['Period'], y=history_df['Value'],\n",
        "                                 mode='lines+markers', name='Original Time Series',\n",
        "                                 line=dict(color='#1f77b4', width=2), marker=dict(color='#1f77b4', size=4, opacity=0.8)))\n",
        "        fig.add_trace(go.Scatter(x=forecast_df['Period'], y=forecast_df['Forecast Value'],\n",
        "                                 mode='lines+markers', name='Forecast',\n",
        "                                 line=dict(color='#d62728', dash='dot', width=2), marker=dict(color='#d62728', size=4, opacity=0.8)))\n",
        "        fig.update_layout(\n",
        "            title=dict(text=f'ARIMA Forecast — ARIMA({int(p)},{int(d)},{int(q)}) for {column_name}', font=dict(size=18, color='#333')),\n",
        "            xaxis_title='Date', yaxis_title='Value', hovermode='x unified', legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
        "            template='plotly_white', margin=dict(l=40, r=40, t=60, b=40),\n",
        "            xaxis=dict(showgrid=True, gridcolor='#e0e0e0'), yaxis=dict(showgrid=True, gridcolor='#e0e0e0')\n",
        "        )\n",
        "\n",
        "        forecast_df_display = forecast_df.copy()\n",
        "        forecast_df_display['Period'] = format_date_series_for_output(forecast_df_display['Period'])\n",
        "        meta = get_filter_suffix_for_filename()\n",
        "        forecast_df_display = maybe_attach_static_cols(forecast_df_display, meta)\n",
        "\n",
        "        fc_path = make_temp_csv(forecast_df_display, \"arima_forecast\", meta)\n",
        "        return forecast_df_display, fig, metrics_text, ds_preview_fig, gr.update(value=fc_path, visible=True), gr.update(visible=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ARIMA Error: {e}\")\n",
        "        return None, None, str(e), make_empty_note_fig(f\"Error: {e}\"), gr.update(visible=False), gr.update(visible=False)\n",
        "\n",
        "# ---------------------------\n",
        "# Dynamic UI Helpers\n",
        "# ---------------------------\n",
        "def math_function_config(func_name):\n",
        "    f = (func_name or \"\").upper()\n",
        "    show_operand = f in [\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"]\n",
        "    show_numeric = f in [\"POWER\", \"ROLLSUM\", \"ROUND\", \"%CHANGE\", \"%EXPGROWTH\", \"CAGR\", \"DIFF\", \"UNIT_CONVERSION\", \"UNIT_MULTIPLIER\", \"CONVERTCUR\", \"DESEASONALIZE\"]\n",
        "    numeric_label_map = {\n",
        "        \"POWER\": \"Exponent\",\n",
        "        \"ROLLSUM\": \"Window (period)\",\n",
        "        \"ROUND\": \"Decimals\",\n",
        "        \"%CHANGE\": \"Period\",\n",
        "        \"%EXPGROWTH\": \"Period\",\n",
        "        \"CAGR\": \"Compounding periods\",\n",
        "        \"DIFF\": \"Period\",\n",
        "        \"UNIT_CONVERSION\": \"Multiplier\",\n",
        "        \"UNIT_MULTIPLIER\": \"Multiplier\",\n",
        "        \"CONVERTCUR\": \"Rate\",\n",
        "        \"DESEASONALIZE\": \"Seasonal Period\"\n",
        "    }\n",
        "    numeric_defaults = {\"POWER\": 2, \"ROLLSUM\": 3, \"ROUND\": 2, \"%CHANGE\": 1, \"%EXPGROWTH\": 1, \"CAGR\": 1, \"DIFF\": 1,\n",
        "                        \"UNIT_CONVERSION\": 1.0, \"UNIT_MULTIPLIER\": 1.0, \"CONVERTCUR\": 1.0, \"DESEASONALIZE\": 12}\n",
        "    show_method = f in [\"CROSSSEC\", \"AGGREGATE\", \"DISAGGREGATE\", \"ACCUMULATE\", \"DESEASONALIZE\"]\n",
        "    method_choices = []\n",
        "    if f == \"CROSSSEC\":\n",
        "        method_choices = [\"mean\", \"sum\", \"max\", \"min\", \"median\", \"std\"]\n",
        "    elif f == \"AGGREGATE\":\n",
        "        method_choices = [\"sum\", \"mean\", \"max\", \"min\", \"first\", \"last\"]\n",
        "    elif f == \"DISAGGREGATE\":\n",
        "        method_choices = [\"linear\", \"spline\", \"ffill\"]\n",
        "    elif f == \"ACCUMULATE\":\n",
        "        method_choices = [\"sum\", \"avg\"]\n",
        "    elif f == \"DESEASONALIZE\":\n",
        "        method_choices = [\"multiplicative\", \"additive\"]\n",
        "\n",
        "    show_frequency = f in [\"AGGREGATE\", \"DISAGGREGATE\"]\n",
        "    show_result_type = f == \"DIVIDE\"\n",
        "    show_time_col = f in [\"AGGREGATE\", \"DISAGGREGATE\", \"DESEASONALIZE\", \"ROLLSUM\", \"%CHANGE\", \"%EXPGROWTH\", \"CAGR\", \"DIFF\", \"ACCUMULATE\"]\n",
        "    show_align_end = f in [\"AGGREGATE\", \"DISAGGREGATE\"]\n",
        "    numeric_label = numeric_label_map.get(f, \"Parameter\")\n",
        "    numeric_default = numeric_defaults.get(f, None)\n",
        "    return {\n",
        "        \"f\": f, \"show_operand\": show_operand, \"show_numeric\": show_numeric,\n",
        "        \"numeric_label\": numeric_label, \"numeric_default\": numeric_default,\n",
        "        \"show_method\": show_method, \"method_choices\": method_choices,\n",
        "        \"show_frequency\": show_frequency, \"show_result_type\": show_result_type,\n",
        "        \"show_time_col\": show_time_col, \"show_align_end\": show_align_end\n",
        "    }\n",
        "\n",
        "def update_functions(category):\n",
        "    category_map = {\n",
        "        \"Basic Arithmetic\": [\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"],\n",
        "        \"Statistical\": [\"SUM\", \"CROSSSEC\", \"ROLLSUM\"],\n",
        "        \"Transformation\": [\"ACCUMULATE\", \"ANTILOG\", \"EXP\", \"LOG\", \"LOG10\", \"POWER\", \"RECIP\", \"ROUND\", \"SQRT\"],\n",
        "        \"Time Series\": [\"%CHANGE\", \"%EXPGROWTH\", \"CAGR\", \"DIFF\", \"AGGREGATE\", \"DISAGGREGATE\", \"DESEASONALIZE\"],\n",
        "        \"Conversion\": [\"UNIT_CONVERSION\", \"UNIT_MULTIPLIER\", \"CONVERTCUR\"]\n",
        "    }\n",
        "    functions = category_map.get(category, [\"ADD\"])\n",
        "    return gr.update(choices=functions, value=functions[0])\n",
        "\n",
        "def on_math_function_change(func_name, operand_source, ds_use_in_math):\n",
        "    global current_filtered_df\n",
        "    cfg = math_function_config(func_name)\n",
        "    f = cfg[\"f\"]\n",
        "\n",
        "    descriptions = {\n",
        "        \"ACCUMULATE\": \"Running total (sum) or running average over the (time) order.\",\n",
        "        \"ADD\": \"Add a number or column to each value.\",\n",
        "        \"SUBTRACT\": \"Subtract a number or column.\",\n",
        "        \"MULTIPLY\": \"Multiply by a number or column.\",\n",
        "        \"DIVIDE\": \"Divide by a number or column. Output can be float, int, or percent.\",\n",
        "        \"ANTILOG\": \"e^x for each value.\",\n",
        "        \"EXP\": \"Exponential (e^x).\",\n",
        "        \"LOG\": \"Natural log (x>0).\",\n",
        "        \"LOG10\": \"Base-10 log (x>0).\",\n",
        "        \"POWER\": \"Raise each value to a power.\",\n",
        "        \"RECIP\": \"1/x (skips zeros).\",\n",
        "        \"ROUND\": \"Round to N decimals.\",\n",
        "        \"SQRT\": \"Square root (x>=0).\",\n",
        "        \"SUM\": \"Column total (scalar).\",\n",
        "        \"ROLLSUM\": \"Moving window sum (size = Window).\",\n",
        "        \"CROSSSEC\": \"Cross-sectional aggregation: mean/sum/max/min/median/std.\",\n",
        "        \"%CHANGE\": \"Percent change over a lag (ordered by time if provided).\",\n",
        "        \"%EXPGROWTH\": \"Percent change over a lag (same units as %CHANGE).\",\n",
        "        \"CAGR\": \"Compound annual growth rate (%) over given periods.\",\n",
        "        \"DIFF\": \"Difference over lag.\",\n",
        "        \"AGGREGATE\": \"Resample to lower frequency (sum/mean/etc.).\",\n",
        "        \"DISAGGREGATE\": \"Up-sample to higher frequency (linear/spline/ffill).\",\n",
        "        \"UNIT_CONVERSION\": \"Multiply by unit factor.\",\n",
        "        \"UNIT_MULTIPLIER\": \"Multiply by factor.\",\n",
        "        \"CONVERTCUR\": \"Convert currency by rate.\",\n",
        "        \"DESEASONALIZE\": \"Remove seasonal pattern (multiplicative or additive).\"\n",
        "    }\n",
        "    desc_update = gr.update(value=descriptions.get(f, \"Select a function to see details.\"), visible=True)\n",
        "\n",
        "    param_help = \"\"\n",
        "    if cfg[\"show_numeric\"]:\n",
        "        if f == \"POWER\": param_help = \"Exponent (e.g., 2 → square).\"\n",
        "        elif f == \"ROLLSUM\": param_help = \"Window length in periods.\"\n",
        "        elif f == \"ROUND\": param_help = \"Decimals.\"\n",
        "        elif f in [\"%CHANGE\", \"%EXPGROWTH\", \"DIFF\"]: param_help = \"Lag period.\"\n",
        "        elif f == \"CAGR\": param_help = \"Number of periods between first and last values.\"\n",
        "        elif f in [\"UNIT_CONVERSION\", \"UNIT_MULTIPLIER\", \"CONVERTCUR\"]: param_help = \"Multiplier/Rate.\"\n",
        "        elif f == \"DESEASONALIZE\": param_help = \"Seasonal period (e.g., 12 for monthly).\"\n",
        "    else:\n",
        "        param_help = \"No numeric parameter required.\"\n",
        "\n",
        "    if cfg[\"show_method\"]:\n",
        "        if f == \"ACCUMULATE\": param_help += \" | Method: sum or avg.\"\n",
        "        elif f == \"CROSSSEC\": param_help += \" | Method: mean/sum/max/min/median/std.\"\n",
        "        elif f == \"AGGREGATE\": param_help += \" | Method: sum/mean/max/min/first/last.\"\n",
        "        elif f == \"DISAGGREGATE\": param_help += \" | Method: linear/spline/ffill.\"\n",
        "        elif f == \"DESEASONALIZE\": param_help += \" | Model: multiplicative/additive.\"\n",
        "    if cfg[\"show_time_col\"]:\n",
        "        param_help += \" | Tip: Choose a Time Column to ensure correct ordering.\"\n",
        "    if ds_use_in_math:\n",
        "        param_help += \" | Using deseasonalized input requires a Time Column.\"\n",
        "\n",
        "    time_choices = detect_datetime_cols(current_filtered_df)\n",
        "    show_time = cfg[\"show_time_col\"] or bool(ds_use_in_math)\n",
        "    return (\n",
        "        gr.update(visible=cfg[\"show_operand\"], value=operand_source if cfg[\"show_operand\"] else None),\n",
        "        gr.update(visible=cfg[\"show_operand\"] and (operand_source == \"value\")),\n",
        "        gr.update(visible=cfg[\"show_operand\"] and (operand_source == \"column\")),\n",
        "        gr.update(visible=cfg[\"show_numeric\"], label=f\"{cfg['numeric_label']} (required)\" if cfg[\"show_numeric\"] else \"Parameter\", value=cfg[\"numeric_default\"], interactive=True),\n",
        "        gr.update(visible=cfg[\"show_method\"], choices=cfg[\"method_choices\"], value=cfg[\"method_choices\"][0] if cfg[\"method_choices\"] else None),\n",
        "        gr.update(visible=cfg[\"show_frequency\"], value=\"M\" if f == \"AGGREGATE\" else \"D\"),\n",
        "        gr.update(visible=cfg[\"show_result_type\"], value=\"float\"),\n",
        "        gr.update(visible=show_time, choices=time_choices, value=(time_choices[0] if (show_time and time_choices) else None)),\n",
        "        gr.update(visible=cfg[\"show_align_end\"], value=True),\n",
        "        desc_update,\n",
        "        gr.update(value=f\"Tip: {param_help}\", visible=True)\n",
        "    )\n",
        "\n",
        "def on_operand_source_change(operand_source, func_name):\n",
        "    cfg = math_function_config(func_name)\n",
        "    return (gr.update(visible=cfg[\"show_operand\"] and (operand_source == \"value\")),\n",
        "            gr.update(visible=cfg[\"show_operand\"] and (operand_source == \"column\")))\n",
        "\n",
        "def on_arima_use_ds_change(use):\n",
        "    return gr.update(visible=bool(use))\n",
        "\n",
        "def on_math_ds_toggle(enable, func_name):\n",
        "    global current_filtered_df\n",
        "    cfg = math_function_config(func_name)\n",
        "    time_choices = detect_datetime_cols(current_filtered_df)\n",
        "    # Show DS params when enabled; also show time column if either function requires it or DS is enabled\n",
        "    show_time = cfg[\"show_time_col\"] or bool(enable)\n",
        "    return (\n",
        "        gr.update(visible=bool(enable), value=12),                    # ds_math_period\n",
        "        gr.update(visible=bool(enable), value=\"multiplicative\"),      # ds_math_model\n",
        "        gr.update(visible=show_time, choices=time_choices, value=(time_choices[0] if (show_time and time_choices) else None))  # time_column\n",
        "    )\n",
        "\n",
        "def toggle_preview():\n",
        "    global preview_visible\n",
        "    preview_visible = not preview_visible\n",
        "    return gr.update(visible=preview_visible), gr.update(value=\"Hide Preview\" if preview_visible else \"Show Preview\")\n",
        "\n",
        "def toggle_filtered():\n",
        "    global filtered_visible\n",
        "    filtered_visible = not filtered_visible\n",
        "    return gr.update(visible=filtered_visible), gr.update(value=\"Hide Filtered Data\" if filtered_visible else \"Show Filtered Data\")\n",
        "\n",
        "def update_table_preview(table_name):\n",
        "    global tables_data\n",
        "    if table_name in tables_data and not tables_data[table_name].empty:\n",
        "        return tables_data[table_name], gr.update(visible=False)\n",
        "    return pd.DataFrame(), gr.update(visible=False)\n",
        "\n",
        "def get_table_preview_only(table_name):\n",
        "    global tables_data\n",
        "    return tables_data.get(table_name, pd.DataFrame())\n",
        "\n",
        "def show_arima_description(show):\n",
        "    return gr.update(visible=show)\n",
        "\n",
        "def _initial_math_updates(numeric_cols, all_cols):\n",
        "    return (\n",
        "        gr.update(choices=[\"Basic Arithmetic\", \"Statistical\", \"Transformation\", \"Time Series\", \"Conversion\"], value=\"Basic Arithmetic\"),\n",
        "        gr.update(choices=[\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"], value=\"ADD\"),\n",
        "        gr.update(choices=numeric_cols, value=numeric_cols[0] if numeric_cols else None),\n",
        "        gr.update(visible=True, value=\"value\"),\n",
        "        gr.update(visible=True, value=None),\n",
        "        gr.update(choices=numeric_cols, visible=False, value=None),\n",
        "        gr.update(visible=False, value=None, label=\"Parameter\", interactive=True),\n",
        "        gr.update(visible=False, choices=[], value=None),\n",
        "        gr.update(visible=False, value=\"M\"),\n",
        "        gr.update(visible=False, value=\"float\"),\n",
        "        gr.update(choices=[], visible=False, value=None),\n",
        "        gr.update(visible=False, value=True),\n",
        "        # New DS controls defaults\n",
        "        gr.update(value=False, visible=True),      # ds_math_use\n",
        "        gr.update(visible=False, value=12),        # ds_math_period\n",
        "        gr.update(visible=False, value=\"multiplicative\"),  # ds_math_model\n",
        "        # Descriptions\n",
        "        gr.update(value=\"\", visible=True),\n",
        "        gr.update(value=\"Tip: Select a function. Fields will appear only when needed.\", visible=True)\n",
        "    )\n",
        "\n",
        "def _arima_ui_updates_for_df(df):\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    time_cols = detect_datetime_cols(df)\n",
        "    default_time = time_cols[0] if time_cols else None\n",
        "    return (\n",
        "        gr.update(choices=numeric_cols, value=numeric_cols[0] if numeric_cols else None),\n",
        "        gr.update(choices=time_cols, value=default_time),\n",
        "        gr.update(value=\"infer\")\n",
        "    )\n",
        "\n",
        "def build_filter_updates_from_df(df):\n",
        "    global filter_columns_map\n",
        "    updates = []\n",
        "    filter_columns_map = {}\n",
        "    cols = df.columns.tolist()\n",
        "    for idx, col in enumerate(cols[:MAX_FILTERS]):\n",
        "        filter_columns_map[idx] = col\n",
        "        unique_vals = df[col].dropna().astype(str).unique().tolist()\n",
        "        updates.append(visible_filter_options(col, unique_vals))\n",
        "    for _ in range(MAX_FILTERS - len(cols[:MAX_FILTERS])):\n",
        "        updates.append(gr.update(visible=False, choices=[], value=None))\n",
        "    return updates\n",
        "\n",
        "def update_all_filters(*selections):\n",
        "    global original_df, filter_columns_map, MAX_UNIQUE_FILTER, active_filter_values\n",
        "    if original_df.empty:\n",
        "        return tuple(gr.update(visible=False, choices=[], value=None) for _ in range(MAX_FILTERS))\n",
        "\n",
        "    df = original_df.copy()\n",
        "    norm_selections = {}\n",
        "    active_filter_values = {}\n",
        "    for i, sel in enumerate(selections):\n",
        "        col = filter_columns_map.get(i)\n",
        "        if not col or col not in df.columns:\n",
        "            continue\n",
        "        if isinstance(sel, (list, tuple, set, np.ndarray, pd.Series)):\n",
        "            sel = next((str(x) for x in sel if str(x) != FILTER_ALL_TOKEN and str(x).strip() != \"\"), None)\n",
        "        sel = None if sel is None or sel == \"\" else str(sel)\n",
        "        norm_selections[i] = sel if sel else FILTER_ALL_TOKEN\n",
        "        active_filter_values[col] = [sel] if sel else [FILTER_ALL_TOKEN]\n",
        "\n",
        "    outs = []\n",
        "    for i in range(MAX_FILTERS):\n",
        "        col_i = filter_columns_map.get(i)\n",
        "        if not col_i or col_i not in df.columns:\n",
        "            outs.append(gr.update(visible=False, choices=[], value=None))\n",
        "            continue\n",
        "\n",
        "        ctx = df\n",
        "        for j in range(i):\n",
        "            col_j = filter_columns_map.get(j)\n",
        "            val_j = norm_selections.get(j, FILTER_ALL_TOKEN)\n",
        "            if col_j and col_j in ctx.columns and val_j != FILTER_ALL_TOKEN:\n",
        "                ctx = ctx[ctx[col_j].astype(str) == val_j]\n",
        "                if ctx.empty:\n",
        "                    break\n",
        "\n",
        "        unique_vals = [] if ctx.empty else ctx[col_i].dropna().astype(str).unique().tolist()\n",
        "        choices = [FILTER_ALL_TOKEN] + sorted(unique_vals)\n",
        "        cur_sel = norm_selections.get(i, FILTER_ALL_TOKEN)\n",
        "        new_value = cur_sel if cur_sel in choices else FILTER_ALL_TOKEN\n",
        "        visible = (len(unique_vals)) <= MAX_UNIQUE_FILTER\n",
        "        outs.append(gr.update(visible=visible, choices=choices, value=new_value, label=str(col_i), interactive=True))\n",
        "\n",
        "    return tuple(outs)\n",
        "\n",
        "# ---------------------------\n",
        "# Callback Functions\n",
        "# ---------------------------\n",
        "def load_file(file):\n",
        "    global original_df, current_filtered_df, preview_visible, filtered_visible, tables_data, active_filter_values\n",
        "    outputs = []\n",
        "    if file is None:\n",
        "        outputs.append(None)  # preview\n",
        "        outputs.extend([gr.update(visible=False, choices=[], value=None) for _ in range(MAX_FILTERS)])\n",
        "        outputs.append(None)  # filtered_df\n",
        "        outputs.append(gr.update(value=\"Show Preview\"))  # toggle_preview_btn\n",
        "        outputs.append(pd.DataFrame())  # stats_df\n",
        "        outputs.append(gr.update(choices=[], value=None))  # arima_column\n",
        "        outputs.append(gr.update(choices=[], value=None))  # arima_time_col\n",
        "        outputs.append(gr.update(value=\"infer\"))           # arima_freq\n",
        "        outputs.append(gr.update(choices=[\"Basic Arithmetic\", \"Statistical\", \"Transformation\", \"Time Series\", \"Conversion\"], value=\"Basic Arithmetic\"))\n",
        "        outputs.append(gr.update(choices=[\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"], value=\"ADD\"))\n",
        "        outputs.append(gr.update(choices=[], value=None))  # math_column\n",
        "        outputs.append(gr.update(visible=False, value=\"value\"))  # operand_source\n",
        "        outputs.append(gr.update(visible=False, value=None))     # operand_value\n",
        "        outputs.append(gr.update(choices=[], visible=False, value=None))  # operand_column\n",
        "        outputs.append(gr.update(visible=False, value=None, label=\"Parameter\", interactive=True))  # numeric_param hidden\n",
        "        outputs.append(gr.update(visible=False, choices=[], value=None))         # method_param\n",
        "        outputs.append(gr.update(visible=False, value=\"M\"))                      # frequency_param\n",
        "        outputs.append(gr.update(visible=False, value=\"float\"))                  # result_type\n",
        "        outputs.append(gr.update(choices=[], visible=False, value=None))         # time_column\n",
        "        outputs.append(gr.update(visible=False, value=True))                     # align_end\n",
        "        # New DS controls\n",
        "        outputs.append(gr.update(value=False, visible=True))                     # ds_math_use\n",
        "        outputs.append(gr.update(visible=False, value=12))                       # ds_math_period\n",
        "        outputs.append(gr.update(visible=False, value=\"multiplicative\"))         # ds_math_model\n",
        "        outputs.append(gr.update(value=\"\", visible=True))                        # math_function_description\n",
        "        outputs.append(gr.update(value=\"Tip: Select a function. Fields will appear only when needed.\", visible=True))  # param_helper\n",
        "        outputs.append(gr.update(choices=[], value=None))  # table_selector\n",
        "        outputs.append(pd.DataFrame())                     # table_preview\n",
        "        outputs.append(gr.update(visible=False))             # modified_csv_download\n",
        "        outputs.append(gr.update(value=\"Show Filtered Data\"))  # toggle_filtered_btn\n",
        "        outputs.append(gr.update(visible=False))  # current_data_download\n",
        "        outputs.append(gr.update(visible=False))  # math_result_download\n",
        "        outputs.append(gr.update(visible=False))  # forecast_download\n",
        "        outputs.append(gr.update(visible=False))  # ds_series_download\n",
        "        return tuple(outputs)\n",
        "\n",
        "    try:\n",
        "        df = safe_load_df(file)\n",
        "        original_df = current_filtered_df = df.copy()\n",
        "        tables_data = {\"Original Table\": df.copy()}  # Changed from \"Table 1\" to \"Original Table\"\n",
        "        active_filter_values = {}\n",
        "        preview_visible = False\n",
        "        filtered_visible = False\n",
        "\n",
        "        outputs.append(df)  # preview\n",
        "        outputs.extend(build_filter_updates_from_df(df))\n",
        "\n",
        "        outputs.append(df)  # filtered_df\n",
        "        outputs.append(gr.update(value=\"Show Preview\"))  # toggle_preview_btn\n",
        "        outputs.append(get_statistics(df))  # stats_df\n",
        "\n",
        "        arima_updates = _arima_ui_updates_for_df(df)\n",
        "        outputs.extend(arima_updates)  # arima_column, arima_time_col, arima_freq\n",
        "\n",
        "        # Math defaults\n",
        "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        math_updates = _initial_math_updates(numeric_cols, df.columns.tolist())\n",
        "        outputs.extend(math_updates)  # includes ds controls now\n",
        "\n",
        "        table_selector_update = gr.update(choices=list(tables_data.keys()), value=list(tables_data.keys())[0] if tables_data else None)\n",
        "        outputs.append(table_selector_update)\n",
        "        outputs.append(df)  # table_preview\n",
        "        outputs.append(gr.update(visible=False))  # modified_csv_download\n",
        "\n",
        "        outputs.append(gr.update(value=\"Show Filtered Data\"))\n",
        "\n",
        "        # current data download auto\n",
        "        meta = get_filter_suffix_for_filename()\n",
        "        cur_df = maybe_attach_static_cols(current_filtered_df, meta)\n",
        "        cur_path = make_temp_csv(cur_df, \"current_data\", meta)\n",
        "        outputs.append(gr.update(value=cur_path, visible=True))  # current_data_download\n",
        "        outputs.append(gr.update(visible=False))  # math_result_download\n",
        "        outputs.append(gr.update(visible=False))  # forecast_download\n",
        "        outputs.append(gr.update(visible=False))  # ds_series_download\n",
        "\n",
        "        return tuple(outputs)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading file: {e}\")\n",
        "        outputs = [None]\n",
        "        outputs.extend([gr.update(visible=False, choices=[], value=None) for _ in range(MAX_FILTERS)])\n",
        "        outputs.append(None)\n",
        "        outputs.append(gr.update(value=\"Show Preview\"))\n",
        "        outputs.append(pd.DataFrame())\n",
        "        outputs.append(gr.update(choices=[], value=None))  # arima_column\n",
        "        outputs.append(gr.update(choices=[], value=None))  # arima_time_col\n",
        "        outputs.append(gr.update(value=\"infer\"))           # arima_freq\n",
        "        outputs.append(gr.update(choices=[\"Basic Arithmetic\", \"Statistical\", \"Transformation\", \"Time Series\", \"Conversion\"], value=\"Basic Arithmetic\"))\n",
        "        outputs.append(gr.update(choices=[\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"], value=\"ADD\"))\n",
        "        outputs.append(gr.update(choices=[], value=None))\n",
        "        outputs.append(gr.update(visible=False, value=\"value\"))\n",
        "        outputs.append(gr.update(visible=False, value=None))\n",
        "        outputs.append(gr.update(choices=[], visible=False, value=None))\n",
        "        outputs.append(gr.update(visible=False, value=None, label=\"Parameter\", interactive=True))\n",
        "        outputs.append(gr.update(visible=False, choices=[], value=None))\n",
        "        outputs.append(gr.update(visible=False, value=\"M\"))\n",
        "        outputs.append(gr.update(visible=False, value=\"float\"))\n",
        "        outputs.append(gr.update(choices=[], visible=False, value=None))\n",
        "        outputs.append(gr.update(visible=False, value=True))\n",
        "        # DS controls defaults on error\n",
        "        outputs.append(gr.update(value=False, visible=True))      # ds_math_use\n",
        "        outputs.append(gr.update(visible=False, value=12))        # ds_math_period\n",
        "        outputs.append(gr.update(visible=False, value=\"multiplicative\"))  # ds_math_model\n",
        "        outputs.append(gr.update(value=\"\", visible=True))\n",
        "        outputs.append(gr.update(value=\"Tip: Select a function. Fields will appear only when needed.\", visible=True))\n",
        "        outputs.append(gr.update(choices=[], value=None))\n",
        "        outputs.append(pd.DataFrame())\n",
        "        outputs.append(gr.update(visible=False))\n",
        "        outputs.append(gr.update(value=\"Show Filtered Data\"))\n",
        "        outputs.append(gr.update(visible=False))  # current_data_download\n",
        "        outputs.append(gr.update(visible=False))  # math_result_download\n",
        "        outputs.append(gr.update(visible=False))  # forecast_download\n",
        "        outputs.append(gr.update(visible=False))  # ds_series_download\n",
        "        return tuple(outputs)\n",
        "\n",
        "def apply_filters(*filters_values):\n",
        "    global original_df, current_filtered_df, tables_data, active_filter_values\n",
        "\n",
        "    df = original_df.copy()\n",
        "    active_filter_values = {}\n",
        "\n",
        "    for i, selected in enumerate(filters_values):\n",
        "        col_name = filter_columns_map.get(i, None)\n",
        "        if not col_name or col_name not in df.columns:\n",
        "            continue\n",
        "\n",
        "        if isinstance(selected, (list, tuple, set, np.ndarray, pd.Series)):\n",
        "            selected = next((str(x) for x in selected if str(x) != FILTER_ALL_TOKEN and str(x).strip() != \"\"), None)\n",
        "        elif selected is not None:\n",
        "            selected = str(selected)\n",
        "\n",
        "        if selected is None or selected == \"\" or selected == FILTER_ALL_TOKEN:\n",
        "            active_filter_values[col_name] = [FILTER_ALL_TOKEN]\n",
        "            continue\n",
        "\n",
        "        col_str = df[col_name].astype(str)\n",
        "        if selected in set(col_str.dropna().unique().tolist()):\n",
        "            df = df[col_str == selected]\n",
        "            active_filter_values[col_name] = [selected]\n",
        "        else:\n",
        "            active_filter_values[col_name] = [FILTER_ALL_TOKEN]\n",
        "\n",
        "    df = coerce_datetime_columns(df)\n",
        "    current_filtered_df = df.copy()\n",
        "    tables_data[\"Filtered Table\"] = df.copy()\n",
        "\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    time_cols = detect_datetime_cols(df)\n",
        "    default_time = time_cols[0] if time_cols else None\n",
        "\n",
        "    math_updates = _initial_math_updates(numeric_cols, df.columns.tolist())\n",
        "    meta = get_filter_suffix_for_filename()\n",
        "    cur_df = maybe_attach_static_cols(current_filtered_df, meta)\n",
        "    cur_path = make_temp_csv(cur_df, \"current_data\", meta)\n",
        "\n",
        "    return (\n",
        "        gr.update(value=df, visible=True),              # filtered_df\n",
        "        gr.update(value=\"Hide Filtered Data\"),          # toggle_filtered_btn\n",
        "        get_statistics(df),                             # stats_df\n",
        "        gr.update(choices=numeric_cols, value=numeric_cols[0] if numeric_cols else None),  # arima_column\n",
        "        gr.update(choices=time_cols, value=default_time),  # arima_time_col\n",
        "        gr.update(value=\"infer\"),  # arima_freq\n",
        "        math_updates[0],  # math_category\n",
        "        math_updates[1],  # math_function\n",
        "        math_updates[2],  # math_column\n",
        "        math_updates[3],  # operand_source\n",
        "        math_updates[4],  # operand_value\n",
        "        math_updates[5],  # operand_column\n",
        "        math_updates[6],  # numeric_param\n",
        "        math_updates[7],  # method_param\n",
        "        math_updates[8],  # frequency_param\n",
        "        math_updates[9],  # result_type\n",
        "        gr.update(choices=time_cols, visible=False, value=None), # time_column\n",
        "        math_updates[11], # align_end\n",
        "        math_updates[12], # ds_math_use\n",
        "        math_updates[13], # ds_math_period\n",
        "        math_updates[14], # ds_math_model\n",
        "        gr.update(choices=list(tables_data.keys()), value=\"Filtered Table\"),\n",
        "        math_updates[15], # math_function_description\n",
        "        math_updates[16],  # param_helper\n",
        "        gr.update(value=cur_path, visible=True)  # current_data_download\n",
        "    )\n",
        "\n",
        "def generate_download_file():\n",
        "    global current_filtered_df\n",
        "    if current_filtered_df.empty:\n",
        "        raise gr.Error(\"No data available to download!\")\n",
        "    meta = get_filter_suffix_for_filename()\n",
        "    path = make_temp_csv(maybe_attach_static_cols(current_filtered_df, meta), \"current_data_manual\", meta)\n",
        "    return gr.update(value=path, visible=True)\n",
        "\n",
        "def clear_all_filters():\n",
        "    global original_df, current_filtered_df, preview_visible, filtered_visible, tables_data, active_filter_values\n",
        "    if original_df.empty:\n",
        "        outs = [None]\n",
        "        outs.extend([gr.update(value=None) for _ in range(MAX_FILTERS)])  # filters\n",
        "        outs.append(None)  # preview\n",
        "        outs.append(gr.update(value=\"Show Preview\"))\n",
        "        outs.append(pd.DataFrame())  # stats\n",
        "        outs.append(gr.update(choices=[], value=None))  # arima_column\n",
        "        outs.append(gr.update(choices=[], value=None))  # arima_time_col\n",
        "        outs.append(gr.update(value=\"infer\"))           # arima_freq\n",
        "        outs.append(gr.update(choices=[\"Basic Arithmetic\", \"Statistical\", \"Transformation\", \"Time Series\", \"Conversion\"], value=\"Basic Arithmetic\"))\n",
        "        outs.append(gr.update(choices=[\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"], value=\"ADD\"))\n",
        "        outs.append(gr.update(choices=[], value=None))  # math_column\n",
        "        outs.append(gr.update(visible=False, value=\"value\"))  # operand_source\n",
        "        outs.append(gr.update(visible=False, value=None))     # operand_value\n",
        "        outs.append(gr.update(choices=[], visible=False, value=None))  # operand_column\n",
        "        outs.append(gr.update(visible=False, value=None, label=\"Parameter\", interactive=True))  # numeric_param hidden\n",
        "        outs.append(gr.update(visible=False, choices=[], value=None))         # method_param\n",
        "        outs.append(gr.update(visible=False, value=\"M\"))                      # frequency_param\n",
        "        outs.append(gr.update(visible=False, value=\"float\"))                  # result_type\n",
        "        outs.append(gr.update(choices=[], visible=False, value=None))         # time_column\n",
        "        outs.append(gr.update(visible=False, value=True))                     # align_end\n",
        "        # DS controls\n",
        "        outs.append(gr.update(value=False, visible=True))      # ds_math_use\n",
        "        outs.append(gr.update(visible=False, value=12))        # ds_math_period\n",
        "        outs.append(gr.update(visible=False, value=\"multiplicative\"))  # ds_math_model\n",
        "        outs.append(gr.update(value=\"\", visible=True))                        # math_function_description\n",
        "        outs.append(gr.update(value=\"Tip: Select a function. Fields will appear only when needed.\", visible=True))  # param_helper\n",
        "        outs.append(gr.update(choices=[], value=None))  # table_selector\n",
        "        outs.append(pd.DataFrame())  # table_preview\n",
        "        outs.append(gr.update(visible=False))  # modified_csv_download\n",
        "        outs.append(gr.update(value=\"Show Filtered Data\"))\n",
        "        outs.append(gr.update(visible=False))  # current_data_download\n",
        "        return tuple(outs)\n",
        "\n",
        "    current_filtered_df = coerce_datetime_columns(original_df.copy())\n",
        "    tables_data = {\"Original Table\": original_df.copy()}  # Changed from \"Table 1\" to \"Original Table\"\n",
        "    active_filter_values = {}\n",
        "    numeric_cols = current_filtered_df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    time_cols = detect_datetime_cols(current_filtered_df)\n",
        "    default_time = time_cols[0] if time_cols else None\n",
        "\n",
        "    preview_visible = False\n",
        "    filtered_visible = False\n",
        "\n",
        "    meta = get_filter_suffix_for_filename()\n",
        "    cur_df = maybe_attach_static_cols(current_filtered_df, meta)\n",
        "    cur_path = make_temp_csv(cur_df, \"current_data\", meta)\n",
        "\n",
        "    outs = [current_filtered_df]  # filtered_df\n",
        "    outs.extend(build_filter_updates_from_df(original_df))  # filters reset to full dataset\n",
        "    outs.append(current_filtered_df)  # preview\n",
        "    outs.append(gr.update(value=\"Show Preview\"))\n",
        "    outs.append(get_statistics(current_filtered_df))\n",
        "    outs.append(gr.update(choices=numeric_cols, value=numeric_cols[0] if numeric_cols else None))\n",
        "    outs.append(gr.update(choices=time_cols, value=default_time))\n",
        "    outs.append(gr.update(value=\"infer\"))\n",
        "\n",
        "    math_updates = _initial_math_updates(numeric_cols, current_filtered_df.columns.tolist())\n",
        "    outs.extend(math_updates)\n",
        "\n",
        "    outs.append(gr.update(choices=list(tables_data.keys()), value=list(tables_data.keys())[0]))  # table_selector\n",
        "    outs.append(current_filtered_df)  # table_preview\n",
        "    outs.append(gr.update(visible=False))  # modified_csv_download\n",
        "    outs.append(gr.update(value=\"Show Filtered Data\"))\n",
        "    outs.append(gr.update(value=cur_path, visible=True))  # current_data_download\n",
        "    return tuple(outs)\n",
        "\n",
        "def get_statistics(df):\n",
        "    if df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df_num = df.copy()\n",
        "    for c in df_num.columns:\n",
        "        if df_num[c].dtype == object:\n",
        "            coerced = robust_to_numeric(df_num[c])\n",
        "            if coerced.notna().mean() >= 0.5:\n",
        "                df_num[c] = coerced\n",
        "\n",
        "    numeric_df = df_num.select_dtypes(include=[np.number])\n",
        "    meaningful_cols = [\n",
        "        col for col in numeric_df.columns\n",
        "        if col.lower() not in [x.lower() for x in EXCLUDED_STAT_COLS] and numeric_df[col].nunique() > 1\n",
        "    ]\n",
        "    stats = []\n",
        "    for col in meaningful_cols:\n",
        "        col_s = numeric_df[col].dropna()\n",
        "        if col_s.empty:\n",
        "            continue\n",
        "        stats.append({\n",
        "            \"Column\": col,\n",
        "            \"Min\": round(col_s.min(), 4),\n",
        "            \"Max\": round(col_s.max(), 4),\n",
        "            \"Average\": round(col_s.mean(), 4),\n",
        "            \"Std Dev\": round(col_s.std(ddof=1), 4) if len(col_s) > 1 else 0.0,\n",
        "            \"Skewness\": round(col_s.skew(), 4) if len(col_s) > 2 else 0.0\n",
        "        })\n",
        "    return pd.DataFrame(stats)\n",
        "\n",
        "# --- Enhanced CSS Styling ---\n",
        "css = \"\"\"\n",
        "/* General Styles */\n",
        "body {\n",
        "    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;\n",
        "    background: linear-gradient(135deg, #f0f4f8 0%, #d9e2ec 100%);\n",
        "    color: #334155;\n",
        "}\n",
        "\n",
        "/* Main Title */\n",
        ".main-title {\n",
        "    text-align: center;\n",
        "    background: linear-gradient(90deg, #2563eb, #3b82f6);\n",
        "    -webkit-background-clip: text;\n",
        "    -webkit-text-fill-color: transparent;\n",
        "    font-size: 2.8em;\n",
        "    font-weight: 800;\n",
        "    margin-bottom: 0.3em;\n",
        "    padding-top: 1em;\n",
        "    text-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        ".subtitle {\n",
        "    text-align: center;\n",
        "    color: #475569;\n",
        "    font-size: 1.3em;\n",
        "    font-weight: 500;\n",
        "    margin-bottom: 2em;\n",
        "}\n",
        "\n",
        "/* Sections */\n",
        ".upload-section, .filter-section, .table-manip-section, .math-panel, .forecast-section {\n",
        "    background: white;\n",
        "    border-radius: 16px;\n",
        "    padding: 24px;\n",
        "    margin-bottom: 24px;\n",
        "    box-shadow: 0 6px 16px rgba(0, 0, 0, 0.08);\n",
        "    transition: transform 0.3s ease, box-shadow 0.3s ease;\n",
        "}\n",
        ".upload-section { border: 2px solid #3b82f6; background: #eff6ff; }\n",
        ".filter-section { border: 2px solid #22c55e; background: #f0fdf4; }\n",
        ".table-manip-section { border: 2px solid #eab308; background: #fefce8; }\n",
        ".math-panel { border: 2px solid #8b5cf6; background: #f5f3ff; }\n",
        ".forecast-section { border: 2px solid #f97316; background: #fff7ed; }\n",
        ".section:hover { transform: translateY(-4px); box-shadow: 0 8px 20px rgba(0, 0, 0, 0.12); }\n",
        "\n",
        "/* Buttons */\n",
        ".button {\n",
        "    border-radius: 8px;\n",
        "    padding: 12px 24px;\n",
        "    font-weight: 600;\n",
        "    transition: all 0.3s ease;\n",
        "    margin: 6px;\n",
        "    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);\n",
        "}\n",
        ".button:hover { transform: translateY(-2px); box-shadow: 0 4px 8px rgba(0, 0, 0, 0.15); }\n",
        ".download-btn { background: linear-gradient(90deg, #22c55e, #4ade80); color: white; }\n",
        ".clear-btn { background: linear-gradient(90deg, #ef4444, #f87171); color: white; }\n",
        ".apply-btn { background: linear-gradient(90deg, #3b82f6, #60a5fa); color: white; }\n",
        ".toggle-btn { background: linear-gradient(90deg, #6b7280, #9ca3af); color: white; }\n",
        ".save-btn { background: linear-gradient(90deg, #f97316, #fb923c); color: white; }\n",
        ".math-btn { background: linear-gradient(90deg, #8b5cf6, #a78bfa); color: white; }\n",
        ".forecast-btn { background: linear-gradient(90deg, #f97316, #fb923c); color: white; }\n",
        "\n",
        "/* Dataframes */\n",
        ".gradio-dataframe table {\n",
        "    border-collapse: separate;\n",
        "    border-spacing: 0 8px;\n",
        "}\n",
        ".gradio-dataframe th, .gradio-dataframe td {\n",
        "    background: #f9fafb;\n",
        "    border: none;\n",
        "    padding: 12px;\n",
        "    border-radius: 6px;\n",
        "}\n",
        ".gradio-dataframe th {\n",
        "    background: #e5e7eb;\n",
        "    font-weight: 600;\n",
        "}\n",
        "\n",
        "/* Dropdowns and Inputs */\n",
        ".gradio-dropdown, .gradio-number, .gradio-radio, .gradio-checkbox {\n",
        "    border-radius: 8px;\n",
        "    border: 1px solid #d1d5db;\n",
        "    box-shadow: 0 1px 2px rgba(0, 0, 0, 0.05);\n",
        "}\n",
        ".gradio-dropdown:hover, .gradio-number:hover {\n",
        "    border-color: #9ca3af;\n",
        "}\n",
        "\n",
        "/* Markdown */\n",
        ".gradio-markdown {\n",
        "    font-size: 1.05em;\n",
        "    line-height: 1.6;\n",
        "}\n",
        "\n",
        "/* Tabs */\n",
        ".gradio-tabs .tabitem {\n",
        "    border-radius: 12px 12px 0 0;\n",
        "    background: #f3f4f6;\n",
        "    padding: 12px 20px;\n",
        "    font-weight: 600;\n",
        "}\n",
        ".gradio-tabs .tabitem.selected {\n",
        "    background: white;\n",
        "    border-bottom: none;\n",
        "    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# --- Gradio UI ---\n",
        "with gr.Blocks(css=css, theme=gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"green\", font=[gr.themes.GoogleFont('Inter'), 'system-ui'])) as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        <h1 class='main-title'>🧮 DataFlow Analytics Pro</h1>\n",
        "        <p class='subtitle'>Advanced Data Transformation, Filtering, and Forecasting Platform</p>\n",
        "        \"\"\",\n",
        "        elem_id=\"main-title\"\n",
        "    )\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.Tab(\"Upload & Filter\"):\n",
        "            with gr.Accordion(\"📂 Upload Data\", open=True, elem_classes=[\"upload-section\"]):\n",
        "                file_input = gr.File(label=\"Upload CSV or Excel File\", file_types=[\".csv\", \".xlsx\", \".xls\"], elem_id=\"file-upload\")\n",
        "                toggle_preview_btn = gr.Button(\"🔍 Preview Data\", elem_classes=[\"toggle-btn\"])\n",
        "                preview = gr.Dataframe(label=\"Data Preview\", interactive=False, visible=False)\n",
        "\n",
        "            with gr.Accordion(\"🗂️ Apply Filters\", open=True, elem_classes=[\"filter-section\"]):\n",
        "                gr.Markdown(\"Select filters progressively from left to right.\")\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    filters = [\n",
        "                        gr.Dropdown(multiselect=False, visible=False, label=f\"Filter {i+1}\", allow_custom_value=False, elem_id=f\"filter-{i}\")\n",
        "                        for i in range(MAX_FILTERS)\n",
        "                    ]\n",
        "\n",
        "            with gr.Row(variant=\"compact\"):\n",
        "                apply_button = gr.Button(\"✅ Apply Filters\", elem_classes=[\"apply-btn\"])\n",
        "                download_btn = gr.Button(\"📥 Snapshot Download\", elem_classes=[\"download-btn\"])\n",
        "                clear_btn = gr.Button(\"🧹 Clear All\", elem_classes=[\"clear-btn\"])\n",
        "\n",
        "            current_data_download = gr.File(label=\"⬇️ Current Data (Auto-Updated)\", visible=False)\n",
        "\n",
        "            toggle_filtered_btn = gr.Button(\"📝 View Filtered Data\", elem_classes=[\"toggle-btn\"])\n",
        "            filtered_df = gr.Dataframe(label=\"Filtered Data (Editable)\", interactive=True, visible=False)\n",
        "            stats_df = gr.Dataframe(label=\"📊 Key Statistics\", interactive=False)\n",
        "\n",
        "            stats_desc = gr.Markdown(\n",
        "                \"\"\"\n",
        "                **Std Dev**: Measures data spread.\n",
        "                **Skewness**: Indicates distribution asymmetry (positive: right-skewed, negative: left-skewed).\n",
        "                \"\"\",\n",
        "                visible=True\n",
        "            )\n",
        "\n",
        "        with gr.Tab(\"Math Operations\"):\n",
        "            with gr.Accordion(\"🔢 Perform Calculations\", open=True, elem_classes=[\"math-panel\"]):\n",
        "                gr.Markdown(\"Choose category and function to transform your data.\")\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    math_category = gr.Dropdown(\n",
        "                        label=\"Category\",\n",
        "                        choices=[\"Basic Arithmetic\", \"Statistical\", \"Transformation\", \"Time Series\", \"Conversion\"],\n",
        "                        value=\"Basic Arithmetic\",\n",
        "                        interactive=True,\n",
        "                        elem_id=\"math-category\"\n",
        "                    )\n",
        "                    math_function = gr.Dropdown(\n",
        "                        label=\"Function\",\n",
        "                        choices=[\"ADD\", \"SUBTRACT\", \"MULTIPLY\", \"DIVIDE\"],\n",
        "                        value=\"ADD\",\n",
        "                        interactive=True,\n",
        "                        elem_id=\"math-function\"\n",
        "                    )\n",
        "                    math_column = gr.Dropdown(label=\"Column\", choices=[], interactive=True, elem_id=\"math-column\")\n",
        "\n",
        "                math_function_description = gr.Markdown(\"\", visible=True)\n",
        "\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    operand_source = gr.Radio(label=\"Operand From\", choices=[\"value\", \"column\"], value=\"value\", visible=True)\n",
        "                    operand_value = gr.Number(label=\"Numeric Value\", visible=True)\n",
        "                    operand_column = gr.Dropdown(label=\"From Column\", choices=[], visible=False)\n",
        "\n",
        "                    numeric_param = gr.Number(label=\"Parameter\", visible=False, interactive=True)\n",
        "                    method_param = gr.Dropdown(label=\"Method\", choices=[], visible=False)\n",
        "                    frequency_param = gr.Dropdown(label=\"Frequency\", choices=[\"D\", \"W\", \"M\", \"Q\", \"Y\"], value=\"M\", visible=False)\n",
        "                    result_type = gr.Radio(label=\"Output Type\", choices=[\"float\", \"int\", \"percent\"], value=\"float\", visible=False)\n",
        "                    time_column = gr.Dropdown(label=\"Time Column\", choices=[], visible=False)\n",
        "                    align_end = gr.Checkbox(label=\"End-Aligned Periods\", value=True, visible=False)\n",
        "\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    ds_math_use = gr.Checkbox(label=\"Deseasonalize Input\", value=False)\n",
        "                    ds_math_period = gr.Number(label=\"Seasonal Period\", value=12, visible=False)\n",
        "                    ds_math_model = gr.Radio(label=\"Model\", choices=[\"multiplicative\", \"additive\"], value=\"multiplicative\", visible=False)\n",
        "\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    add_to_data_chk = gr.Checkbox(label=\"Add to Dataset\", value=False)\n",
        "                    result_col_name = gr.Textbox(label=\"New Column Name\", placeholder=\"e.g., Result_Column\")\n",
        "                    show_original_chk = gr.Checkbox(label=\"Overlay Original\", value=True)\n",
        "\n",
        "                param_helper = gr.Markdown(\"Tip: Select a function. Fields will appear only when needed.\", visible=True)\n",
        "\n",
        "                math_btn = gr.Button(\"▶️ Apply Function\", elem_classes=[\"math-btn\"])\n",
        "                math_output = gr.Dataframe(label=\"Results Table\", interactive=False)\n",
        "                math_status = gr.Textbox(label=\"Operation Status\", interactive=False)\n",
        "                math_result_download = gr.File(label=\"⬇️ Download Results\", visible=False)\n",
        "                math_visualization = gr.Plot(label=\"Visualization\")\n",
        "\n",
        "        with gr.Tab(\"ARIMA Forecasting\"):\n",
        "            with gr.Accordion(\"🔮 Generate Forecasts\", open=True, elem_classes=[\"forecast-section\"]):\n",
        "                gr.Markdown(\"ARIMA model with optional deseasonalization for better accuracy.\")\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    arima_column = gr.Dropdown(label=\"Forecast Column\", choices=[], interactive=True)\n",
        "                    arima_time_col = gr.Dropdown(label=\"Time Column\", choices=[], interactive=True)\n",
        "                    arima_freq = gr.Dropdown(label=\"Frequency\", choices=[\"infer\", \"D\", \"W\", \"M\", \"Q\", \"Y\"], value=\"infer\", interactive=True)\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    arima_steps = gr.Number(label=\"Steps Ahead\", value=5, interactive=True)\n",
        "                    arima_p = gr.Number(label=\"AR (p)\", value=1, interactive=True)\n",
        "                    arima_d = gr.Number(label=\"Diff (d)\", value=1, interactive=True)\n",
        "                    arima_q = gr.Number(label=\"MA (q)\", value=1, interactive=True)\n",
        "                    arima_btn = gr.Button(\"🚀 Run Forecast\", elem_classes=[\"forecast-btn\"])\n",
        "\n",
        "                arima_param_help = gr.Markdown(\n",
        "                    \"**ARIMA Guide**: p (autoregression), d (differencing), q (moving average). Start with low values.\",\n",
        "                    visible=True\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"#### Deseasonalization Settings\")\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    ds_overlay = gr.Checkbox(label=\"Show Deseasonalized Overlay\", value=True)\n",
        "                    ds_period = gr.Number(label=\"Period (e.g., 12)\", value=12)\n",
        "                    ds_model = gr.Radio(label=\"Type\", choices=[\"multiplicative\", \"additive\"], value=\"multiplicative\")\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    ds_use_arima = gr.Checkbox(label=\"Use for ARIMA Input\", value=False)\n",
        "                    ds_reseason_out = gr.Checkbox(label=\"Re-seasonalize Output\", value=True, visible=False)\n",
        "\n",
        "                with gr.Column():\n",
        "                    arima_output = gr.Dataframe(label=\"Forecast Table\", interactive=False)\n",
        "                    arima_plot = gr.Plot(label=\"Forecast Chart\")\n",
        "                    ds_preview_plot = gr.Plot(label=\"Deseason Preview\")\n",
        "                    arima_metrics = gr.Textbox(label=\"Accuracy Metrics\", interactive=False)\n",
        "                    forecast_download = gr.File(label=\"⬇️ Download Forecast\", visible=False)\n",
        "                    ds_series_download = gr.File(label=\"⬇️ Download Deseasonalized\", visible=False)\n",
        "\n",
        "        with gr.Tab(\"Table Editor\"):\n",
        "            with gr.Accordion(\"✏️ Edit Tables\", open=True, elem_classes=[\"table-manip-section\"]):\n",
        "                gr.Markdown(\"Select and edit tables interactively.\")\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    table_selector = gr.Dropdown(\n",
        "                        label=\"Choose Table\",\n",
        "                        choices=[],\n",
        "                        interactive=True\n",
        "                    )\n",
        "                    update_preview_btn = gr.Button(\n",
        "                        \"🔄 Refresh\",\n",
        "                        elem_classes=[\"toggle-btn\"]\n",
        "                    )\n",
        "                table_preview = gr.Dataframe(\n",
        "                    label=\"Editable Table\",\n",
        "                    interactive=True,\n",
        "                    wrap=True,\n",
        "                    datatype=[\"str\", \"number\", \"bool\", \"date\", \"datetime\"]\n",
        "                )\n",
        "                with gr.Row(variant=\"compact\"):\n",
        "                    save_changes_btn = gr.Button(\n",
        "                        \"💾 Save Edits\",\n",
        "                        elem_classes=[\"save-btn\"]\n",
        "                    )\n",
        "                    modification_status = gr.Textbox(\n",
        "                        label=\"Edit Status\",\n",
        "                        interactive=False,\n",
        "                        value=\"No changes applied yet\"\n",
        "                    )\n",
        "                modified_csv_download = gr.File(\n",
        "                    label=\"⬇️ Download Edited Table\",\n",
        "                    interactive=False,\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "    download_output = gr.File(label=\"⬇️ Manual Snapshot\", visible=False)\n",
        "\n",
        "    # --- Event Handlers ---\n",
        "\n",
        "    # File load\n",
        "    file_input.change(\n",
        "        fn=load_file,\n",
        "        inputs=file_input,\n",
        "        outputs=[\n",
        "            preview,\n",
        "            *filters,\n",
        "            filtered_df,\n",
        "            toggle_preview_btn,\n",
        "            stats_df,\n",
        "            arima_column,\n",
        "            arima_time_col,\n",
        "            arima_freq,\n",
        "            math_category, math_function, math_column,\n",
        "            operand_source, operand_value, operand_column,\n",
        "            numeric_param, method_param, frequency_param, result_type, time_column, align_end,\n",
        "            ds_math_use, ds_math_period, ds_math_model,\n",
        "            math_function_description, param_helper,\n",
        "            table_selector, table_preview, modified_csv_download,\n",
        "            toggle_filtered_btn,\n",
        "            current_data_download,\n",
        "            math_result_download,\n",
        "            forecast_download,\n",
        "            ds_series_download\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Progressive cascading filters (left-to-right)\n",
        "    for f in filters:\n",
        "        f.change(\n",
        "            fn=update_all_filters,\n",
        "            inputs=filters,\n",
        "            outputs=filters\n",
        "        )\n",
        "\n",
        "    # Apply filters\n",
        "    apply_button.click(\n",
        "        fn=apply_filters,\n",
        "        inputs=filters,\n",
        "        outputs=[\n",
        "            filtered_df, toggle_filtered_btn, stats_df,\n",
        "            arima_column, arima_time_col, arima_freq,\n",
        "            math_category, math_function, math_column,\n",
        "            operand_source, operand_value, operand_column,\n",
        "            numeric_param, method_param, frequency_param, result_type,\n",
        "            time_column, align_end,\n",
        "            ds_math_use, ds_math_period, ds_math_model,\n",
        "            table_selector,\n",
        "            math_function_description, param_helper,\n",
        "            current_data_download\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Manual download snapshot\n",
        "    download_btn.click(fn=generate_download_file, outputs=[download_output])\n",
        "\n",
        "    # Clear filters\n",
        "    clear_btn.click(\n",
        "        fn=clear_all_filters,\n",
        "        outputs=[\n",
        "            filtered_df, *filters, preview, toggle_preview_btn, stats_df,\n",
        "            arima_column, arima_time_col, arima_freq,\n",
        "            math_category, math_function, math_column,\n",
        "            operand_source, operand_value, operand_column,\n",
        "            numeric_param, method_param, frequency_param, result_type, time_column, align_end,\n",
        "            ds_math_use, ds_math_period, ds_math_model,\n",
        "            math_function_description, param_helper,\n",
        "            table_selector, table_preview, modified_csv_download, toggle_filtered_btn,\n",
        "            current_data_download\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Math category change\n",
        "    math_category.change(\n",
        "        fn=update_functions,\n",
        "        inputs=math_category,\n",
        "        outputs=math_function\n",
        "    )\n",
        "\n",
        "    # Math function UI dynamics (now aware of DS toggle)\n",
        "    math_function.change(\n",
        "        fn=on_math_function_change,\n",
        "        inputs=[math_function, operand_source, ds_math_use],\n",
        "        outputs=[\n",
        "            operand_source, operand_value, operand_column,\n",
        "            numeric_param, method_param, frequency_param, result_type, time_column, align_end,\n",
        "            math_function_description, param_helper\n",
        "        ]\n",
        "    )\n",
        "    operand_source.change(\n",
        "        fn=on_operand_source_change,\n",
        "        inputs=[operand_source, math_function],\n",
        "        outputs=[operand_value, operand_column]\n",
        "    )\n",
        "\n",
        "    # Toggle: DS for Math\n",
        "    ds_math_use.change(\n",
        "        fn=on_math_ds_toggle,\n",
        "        inputs=[ds_math_use, math_function],\n",
        "        outputs=[ds_math_period, ds_math_model, time_column]\n",
        "    )\n",
        "\n",
        "    # ARIMA DS reseason visibility\n",
        "    ds_use_arima.change(\n",
        "        fn=on_arima_use_ds_change,\n",
        "        inputs=ds_use_arima,\n",
        "        outputs=ds_reseason_out\n",
        "    )\n",
        "\n",
        "    # Apply math function\n",
        "    math_btn.click(\n",
        "        fn=apply_math_function,\n",
        "        inputs=[math_function, math_column, operand_source, operand_value, operand_column,\n",
        "                numeric_param, method_param, frequency_param, result_type, time_column, align_end,\n",
        "                ds_math_use, ds_math_period, ds_math_model,\n",
        "                add_to_data_chk, result_col_name, show_original_chk],\n",
        "        outputs=[\n",
        "            math_output, math_status, math_result_download, current_data_download, math_visualization,\n",
        "            arima_column, arima_time_col, math_column, table_selector, table_preview\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # ARIMA + Deseasonalization (updated)\n",
        "    arima_btn.click(\n",
        "        fn=run_arima,\n",
        "        inputs=[arima_column, arima_steps, arima_p, arima_d, arima_q, arima_time_col, arima_freq,\n",
        "                ds_overlay, ds_period, ds_model, ds_use_arima, ds_reseason_out],\n",
        "        outputs=[arima_output, arima_plot, arima_metrics, ds_preview_plot, forecast_download, ds_series_download]\n",
        "    )\n",
        "\n",
        "    # Toggle preview/filtered visibility\n",
        "    toggle_preview_btn.click(\n",
        "        fn=toggle_preview,\n",
        "        inputs=None,\n",
        "        outputs=[preview, toggle_preview_btn]\n",
        "    )\n",
        "    toggle_filtered_btn.click(\n",
        "        fn=toggle_filtered,\n",
        "        inputs=None,\n",
        "        outputs=[filtered_df, toggle_filtered_btn]\n",
        "    )\n",
        "\n",
        "    # Table manipulation\n",
        "    table_selector.change(\n",
        "        fn=update_table_preview,\n",
        "        inputs=[table_selector],\n",
        "        outputs=[table_preview, modified_csv_download]\n",
        "    )\n",
        "    update_preview_btn.click(\n",
        "        fn=get_table_preview_only,\n",
        "        inputs=[table_selector],\n",
        "        outputs=[table_preview]\n",
        "    )\n",
        "    save_changes_btn.click(\n",
        "        fn=handle_table_modification,\n",
        "        inputs=[table_selector, table_preview],\n",
        "        outputs=[modified_csv_download, modification_status, current_data_download]\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "9ddKbAIDgH66",
        "outputId": "a65ccc11-1c1f-4579-a847-9e019aa2d3be"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://64fb8ef75893c3ef2d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://64fb8ef75893c3ef2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}